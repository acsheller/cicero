# Ollama

In this project `Ollama` will serve up local models for use by `Fabric`.


Ollama is used to server local LLMs for Fabric. While something like GPT4o could be used, when learning it is more afordable to use local models.  One can always switch the default model for 