{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49d0a1a-f8a3-4816-802e-4580a1673cd0",
   "metadata": {},
   "source": [
    "# NRMS Model\n",
    "\n",
    "NRMS stands for `Neural News Reccomendaiton with Multi-head Self-Attention`.  The reference to the paper is provided below. \n",
    "\n",
    "---\n",
    "\n",
    "## Understand the MIND dataset\n",
    "The MIND dataset consists of several key files:\n",
    "\n",
    "- news.tsv: Contains news articles and their metadata (news ID, category, subcategory, title, abstract, etc.).\n",
    "- behaviors.tsv: Contains user interaction data, including the history of news articles clicked and the impressions list (clicked or not clicked).\n",
    "\n",
    "The NRMS model uses this data to learn user preferences based on click history.\n",
    "\n",
    "---\n",
    "\n",
    "## Getting setup\n",
    "Create the virtural environment\n",
    "```bash\n",
    "\n",
    "python -m venv nrms\n",
    "\n",
    "```\n",
    "\n",
    "Edit your .bashrc and add an alias:\n",
    "\n",
    "```.bash\n",
    "\n",
    "alias nrms='source ~/nrms/bin/activate'\n",
    "\n",
    "```\n",
    "\n",
    "Source the .bashrc file and activate the nrms enviroment'\n",
    "\n",
    "```.bash\n",
    "\n",
    "source .bashrc\n",
    "\n",
    "nrms\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Install the Python Modules Needed\n",
    "\n",
    "Note, you should match the cuda version of torch to what you have installed.\n",
    "```.bash\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "pip install transformers pandas numpy nltk scikit-learn tqdm gensim matplotlib\n",
    "\n",
    "pip install jupyterlab\n",
    "\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed8aee-0daa-442f-8b06-f244e871f4e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Do the imports and ensure it works\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3b683-d30e-4602-a195-e8192deafc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb622dff-5e9a-4646-ada1-f64286d775db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define Parameters\n",
    "# We start by defining some important parameters for our model such as embedding dimensions, number of attention heads, batch size, etc.\n",
    "embedding_dim = 768  # Using BERT embedding dimensions\n",
    "attention_heads = 8  # Number of attention heads in multi-head attention\n",
    "batch_size = 64  # Number of samples in each batch\n",
    "num_epochs = 10  # Number of epochs to train the model\n",
    "learning_rate = 0.001  # Learning rate for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14dd7b9-d998-4e16-b54b-676975d2c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load MIND Dataset\n",
    "# Now, we will load the MIND dataset, which contains user behaviors and news articles.\n",
    "# We have the datasets already downloaded in ~/datasets/MINDlarge and ~/datasets/MINDsmall.\n",
    "mind_large = '~/datasets/MINDlarge'\n",
    "mind_large_train = mind_large + '/train/'\n",
    "mind_large_dev = mind_large + '/dev/'  # Development -- help tune hyper-parameter \n",
    "mind_large_test = mind_large + '/test/'\n",
    "\n",
    "mind_small = '~/datasets/MINDsmall'\n",
    "mind_small_train = mind_small + '/train/'\n",
    "mind_small_dev = mind_small + '/dev/'\n",
    "\n",
    "dataset_path = mind_small_train\n",
    "\n",
    "# Load training data\n",
    "df_behaviors = pd.read_csv(f\"{dataset_path}behaviors.tsv\", sep=\"\\t\", names=[\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"])\n",
    "df_news = pd.read_csv(f\"{dataset_path}news.tsv\", sep=\"\\t\", names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be8b66-0212-444b-8598-9f7e7c058f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preprocessing\n",
    "# Check for missing values in the title column and remove them.\n",
    "df_news = df_news[df_news['title'].notna()].copy()\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# The next step is to preprocess the news dataset. We tokenize the news titles using BERT tokenizer.\n",
    "def preprocess_news(news_df):\n",
    "    # Tokenize the news title using the BERT tokenizer\n",
    "    news_df.loc[:, 'title_tokens'] = news_df['title'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=50, truncation=True))\n",
    "    return news_df\n",
    "\n",
    "df_news = preprocess_news(df_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780cfcb-1192-4529-859e-d294c1e6254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Vocabulary\n",
    "# BERT tokenizer already provides the vocabulary, so no need to create a custom vocabulary.\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a879182-33b4-4be1-bd85-edc3264d9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Dataset and DataLoader Classes\n",
    "# We define a custom Dataset class to load and serve the data to the model. This class will convert news titles and user behaviors into tensors.\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df_behaviors, df_news):\n",
    "        self.behaviors = df_behaviors\n",
    "        self.news = df_news\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract user history and impressions from the behaviors dataset.\n",
    "        user_history = self.behaviors.iloc[idx]['history'].split()\n",
    "        impressions = self.behaviors.iloc[idx]['impressions'].split()\n",
    "        # Get the tokenized titles for each news article in the user's history.\n",
    "        news_titles = []\n",
    "        for news_id in user_history:\n",
    "            matching_news = self.news[self.news['news_id'] == news_id]\n",
    "            if not matching_news.empty:\n",
    "                news_titles.append(matching_news['title_tokens'].values[0])\n",
    "        if not news_titles:\n",
    "            # If no valid news articles are found, return an empty tensor with padding\n",
    "            news_titles = [[0]]\n",
    "        return torch.tensor(news_titles, dtype=torch.long), torch.tensor([1 if '1' in imp else 0 for imp in impressions])\n",
    "\n",
    "# Create the dataset and dataloader for training.\n",
    "train_dataset = NewsDataset(df_behaviors, df_news)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e10c2-0aee-44c7-8c67-8fd8687babb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define the NRMS Model\n",
    "# Here, we define the NRMS model. The model uses BERT embeddings and a multi-head attention mechanism to capture the relationships between words.\n",
    "class NRMS(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_heads):\n",
    "        super(NRMS, self).__init__()\n",
    "        # BERT model to get embeddings\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Multi-head attention layer to capture interactions between words.\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, attention_heads)\n",
    "        # Fully connected layer to produce the final output.\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert input sequences to embeddings using BERT.\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(x)[0]  # Extract the last hidden state from BERT\n",
    "        x = x.permute(1, 0, 2)  # Convert to (SeqLen, Batch, EmbeddingDim)\n",
    "        # Apply multi-head attention.\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        # Average pooling over sequence length and pass through a fully connected layer.\n",
    "        out = self.fc(attn_output.mean(dim=0))\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87979b24-37c7-4573-b694-1d4a4adab340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Training Loop\n",
    "# We define the training loop to train the NRMS model on the MIND dataset.\n",
    "model = NRMS(embedding_dim, attention_heads)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer.\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for news_tokens, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        # Move data to the appropriate device (CPU or GPU).\n",
    "        news_tokens, labels = news_tokens.to(device), labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()  # Clear previous gradients.\n",
    "        outputs = model(news_tokens)  # Forward pass through the model.\n",
    "        loss = criterion(outputs.view(-1), labels.view(-1))  # Calculate loss.\n",
    "        loss.backward()  # Backpropagate the loss.\n",
    "        optimizer.step()  # Update model parameters.\n",
    "        epoch_loss += loss.item()\n",
    "    # Print the average loss for the epoch.\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Step 8: Save the Model\n",
    "# Finally, save the trained model so it can be used for inference or further training.\n",
    "torch.save(model.state_dict(), 'nrms_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb1ea4-b13b-479e-9568-fb130ce8384b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab6545d-04a4-4521-b918-cab868602d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e3a372edb6464493c6d6b111dc0f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/9811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m news_tokens, attention_mask, labels \u001b[38;5;241m=\u001b[39m news_tokens\u001b[38;5;241m.\u001b[39mto(device), attention_mask\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    139\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass through the model.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Calculate loss.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagate the loss.\u001b[39;00m\n",
      "File \u001b[0;32m~/nrms/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nrms/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 116\u001b[0m, in \u001b[0;36mNRMS.forward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attention_mask):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Convert input sequences to embeddings using BERT.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 116\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract the last hidden state from BERT\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Convert to (SeqLen, Batch, EmbeddingDim)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Apply multi-head attention.\u001b[39;00m\n",
      "File \u001b[0;32m~/nrms/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nrms/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nrms/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1064\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m   1065\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Step 1: Define Parameters\n",
    "# We start by defining some important parameters for our model such as embedding dimensions, number of attention heads, batch size, etc.\n",
    "embedding_dim = 768  # Using BERT embedding dimensions\n",
    "attention_heads = 8  # Number of attention heads in multi-head attention\n",
    "batch_size = 16  # Number of samples in each batch\n",
    "num_epochs = 10  # Number of epochs to train the model\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "\n",
    "# Step 2: Load MIND Dataset\n",
    "# Now, we will load the MIND dataset, which contains user behaviors and news articles.\n",
    "# We have the datasets already downloaded in ~/datasets/MINDlarge and ~/datasets/MINDsmall.\n",
    "mind_large = '~/datasets/MINDlarge'\n",
    "mind_large_train = mind_large + '/train/'\n",
    "mind_large_dev = mind_large + '/dev/'  # Development -- help tune hyper-parameter \n",
    "mind_large_test = mind_large + '/test/'\n",
    "\n",
    "mind_small = '~/datasets/MINDsmall'\n",
    "mind_small_train = mind_small + '/train/'\n",
    "mind_small_dev = mind_small + '/dev/'\n",
    "\n",
    "dataset_path = mind_small_train\n",
    "\n",
    "# Load training data\n",
    "df_behaviors = pd.read_csv(f\"{dataset_path}behaviors.tsv\", sep=\"\\t\", names=[\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"])\n",
    "df_news = pd.read_csv(f\"{dataset_path}news.tsv\", sep=\"\\t\", names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"entity\"])\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "# Check for missing values in the title column and remove them.\n",
    "df_news = df_news[df_news['title'].notna()].copy()\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# The next step is to preprocess the news dataset. We tokenize the news titles using BERT tokenizer.\n",
    "def preprocess_news(news_df):\n",
    "    # Tokenize the news title using the BERT tokenizer\n",
    "    news_df.loc[:, 'title_tokens'] = news_df['title'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=50, truncation=True))\n",
    "    return news_df\n",
    "\n",
    "df_news = preprocess_news(df_news)\n",
    "\n",
    "# Step 4: Create Vocabulary\n",
    "# BERT tokenizer already provides the vocabulary, so no need to create a custom vocabulary.\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Step 5: Dataset and DataLoader Classes\n",
    "# We define a custom Dataset class to load and serve the data to the model. This class will convert news titles and user behaviors into tensors.\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df_behaviors, df_news):\n",
    "        self.behaviors = df_behaviors\n",
    "        self.news = df_news\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract user history and impressions from the behaviors dataset.\n",
    "        user_history = str(self.behaviors.iloc[idx]['history']).split()\n",
    "        impressions = str(self.behaviors.iloc[idx]['impressions']).split()\n",
    "        # Get the tokenized titles for each news article in the user's history.\n",
    "        news_titles = []\n",
    "        for news_id in user_history:\n",
    "            matching_news = self.news[self.news['news_id'] == news_id]\n",
    "            if not matching_news.empty:\n",
    "                news_titles.append(matching_news['title_tokens'].values[0])\n",
    "        if not news_titles:\n",
    "            # If no valid news articles are found, return a tensor filled with padding\n",
    "            news_titles = [[0]]\n",
    "        # Convert list of token lists to tensors\n",
    "        news_titles = [torch.tensor(tokens, dtype=torch.long) for tokens in news_titles]\n",
    "        labels = torch.tensor([1 if '1' in imp else 0 for imp in impressions], dtype=torch.float)\n",
    "        return news_titles, labels\n",
    "\n",
    "# Custom collate function to handle batches with varying sequence lengths\n",
    "def collate_fn(batch):\n",
    "    news_titles_batch, labels_batch = zip(*batch)\n",
    "    # Pad each list of news titles independently\n",
    "    padded_news_titles = [pad_sequence(news, batch_first=True, padding_value=0) for news in news_titles_batch]\n",
    "    # Stack all padded news titles into a batch\n",
    "    news_titles_padded = pad_sequence(padded_news_titles, batch_first=True, padding_value=0)\n",
    "    # Pad labels to ensure consistent batch size\n",
    "    labels_padded = pad_sequence(labels_batch, batch_first=True, padding_value=0)\n",
    "    # Create attention masks\n",
    "    attention_mask = (news_titles_padded != 0).long()\n",
    "    return news_titles_padded, attention_mask, labels_padded\n",
    "\n",
    "# Create the dataset and dataloader for training.\n",
    "train_dataset = NewsDataset(df_behaviors, df_news)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Step 6: Define the NRMS Model\n",
    "# Here, we define the NRMS model. The model uses BERT embeddings and a multi-head attention mechanism to capture the relationships between words.\n",
    "class NRMS(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_heads):\n",
    "        super(NRMS, self).__init__()\n",
    "        # BERT model to get embeddings\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Multi-head attention layer to capture interactions between words.\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, attention_heads)\n",
    "        # Fully connected layer to produce the final output.\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Convert input sequences to embeddings using BERT.\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(x, attention_mask=attention_mask)[0]  # Extract the last hidden state from BERT\n",
    "        x = x.permute(1, 0, 2)  # Convert to (SeqLen, Batch, EmbeddingDim)\n",
    "        # Apply multi-head attention.\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        # Average pooling over sequence length and pass through a fully connected layer.\n",
    "        out = self.fc(attn_output.mean(dim=0))\n",
    "        return torch.sigmoid(out).squeeze()\n",
    "\n",
    "# Step 7: Training Loop\n",
    "# We define the training loop to train the NRMS model on the MIND dataset.\n",
    "model = NRMS(embedding_dim, attention_heads)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer.\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for news_tokens, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        # Move data to the appropriate device (CPU or GPU).\n",
    "        news_tokens, attention_mask, labels = news_tokens.to(device), attention_mask.to(device), labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()  # Clear previous gradients.\n",
    "        outputs = model(news_tokens, attention_mask)  # Forward pass through the model.\n",
    "        loss = criterion(outputs.view(-1), labels.view(-1))  # Calculate loss.\n",
    "        loss.backward()  # Backpropagate the loss.\n",
    "        optimizer.step()  # Update model parameters.\n",
    "        epoch_loss += loss.item()\n",
    "    # Print the average loss for the epoch.\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Step 8: Save the Model\n",
    "# Finally, save the trained model so it can be used for inference or further training.\n",
    "torch.save(model.state_dict(), 'nrms_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c78d6-9abf-421e-9442-1c1e26bfd7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42b3378-bc93-413b-8cab-ea9581a73938",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2cc39b-cd95-4be9-9520-bfca4eead210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
