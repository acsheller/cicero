version: '3.8'

networks:
  cicero:
    driver: bridge

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    runtime: nvidia  # GPU support
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - cicero
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_models:/root/.ollama
    stdin_open: true
    tty: true


  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    runtime: nvidia  # GPU support
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_AUTH=False
      - DATA_DIR=/app/backend/data
      - DEFAULT_MODELS=llama3.2    
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - cicero
    ports:
      - "8080:8080"
    volumes:
      - ./openwebui-data/data:/app/backend/data # Persist open-webui data
    restart: always
    stdin_open: true
    tty: true

  suber:
    build:
      context: .
      dockerfile: Dockerfile.suber
    container_name: suber-container
    runtime: nvidia  # GPU support
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - cicero
    volumes:
      - ./SUBERX:/app/SUBERX  # Mount SUBERX directory to the container
      - ./datasets:/app/datasets
    ports:
      - "8889:8889"  # Expose JupyterLab port for suber
    stdin_open: true  # Keep STDIN open
    tty: true  # Allocate a pseudo-TTY
    ipc: host  # Equivalent to --ipc=host
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864



  nrms:
    build:
      context: .
      dockerfile: Dockerfile.nrms
    container_name: nrms-container
    runtime: nvidia  # For GPU support
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ./nrms:/app/nrms  # Mount your local nrms folder to the container
      - ./datasets:/app/datasets  # Mount directory for data persistence
    ports:
      - "8888:8888"  # Expose JupyterLab port
    stdin_open: true  # Keep STDIN open
    tty: true  # Allocate a pseudo-TTY

  pyai:
    build:
      context: .
      dockerfile: Dockerfile.pyai
    container_name: pyai-container
    runtime: nvidia  # GPU support
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - cicero
    volumes:
      - ./pyai:/app/jupyter  # Mount SUBERX directory to the container
      - ./datasets:/app/datasets
    ports:
      - "8890:8890"  # Expose JupyterLab port for suber
    stdin_open: true  # Keep STDIN open
    tty: true  # Allocate a pseudo-TTY
    ipc: host  # Equivalent to --ipc=host
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864