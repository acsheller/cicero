{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57f4dc5-e2d4-48af-b634-a36f1b0ba6bc",
   "metadata": {},
   "source": [
    "# CICERO -- SUBER with NRMS as the model to be trained\n",
    "\n",
    "How can we take the NRMS model and use it as input for SUBER?  By that I mean, how it provides the data in the form that the original NRMS model uses \n",
    "\n",
    "The NRMS  data comes out of the recommenders module itself. It's installed in the contianer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc102976-1a42-45a9-b451-a81e5341ead2",
   "metadata": {},
   "source": [
    "## do imports and check things out.\n",
    "\n",
    "Are we using a GPU?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fca9f9-0385-4053-9f54-3f78777d859a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff884381-7536-4fcf-ad55-c1ba57a1204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
      "Tensorflow version: 2.15.1\n",
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU is available and TensorFlow is using it.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove warnings\n",
    "import os\n",
    "os.environ['TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# List available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Check if a GPU is detected\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. TensorFlow is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af570-5c83-4b45-bd09-41ccf60f9bea",
   "metadata": {},
   "source": [
    "## Prepare Parameters\n",
    "\n",
    "Adjust these as needed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5974d7-912e-4bff-b82e-6eb54aef5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# My modification for MINDSMALL as I mount in the data into the container.  Further the small dataset is not accessable anymore.\n",
    "\n",
    "# Options: MINDdemo, MINDsmall, MINDlarge\n",
    "\n",
    "\n",
    "\n",
    "MIND_type = 'MINDdemo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d489c-d20a-466a-9272-1384108a3032",
   "metadata": {},
   "source": [
    "## Specify the dataset to use\n",
    "\n",
    "There are three in the original notebook: `demo`, `small` and `large`.\n",
    "\n",
    "From the [original  notebook](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb),  the `demo` set is 5000 samples of the `small` dataset.\n",
    "\n",
    "I was able to download the original demo but that's not really needed. A sample of 5000 is good for showing the algorithm works but won't train it well enough.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29954d8-37fb-422d-9b3f-fc874fe01510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmpdir = TemporaryDirectory()\n",
    "#data_path = tmpdir.name\n",
    "data_path_base=\"/app/datasets/\"\n",
    "data_path = data_path_base + MIND_type\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path is {data_path}\")\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path_base, \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path_base, \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path_base, \"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path_base, r'nrms.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610d53a-c6a9-44e0-a82f-3b38838c3b0f",
   "metadata": {},
   "source": [
    "## Download Glove embeddings\n",
    "\n",
    "The original NRMS used glove embeddings, this is how they are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9bf3e-275d-4371-9776-639ce23b1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.mind import download_and_extract_glove\n",
    "\n",
    "# Download and extract GloVe embeddings\n",
    "glove_dir = '/app/datasets/glove_embeddings'\n",
    "download_and_extract_glove(glove_dir)\n",
    "print(f\"GloVe embeddings extracted to: {glove_dir}\")\n",
    "\n",
    "# Specify the directory where NLTK data should be downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "#nltk.data.path.append('app/SUBERX/datasets/nltk_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7569bae-6674-4fa8-9c33-4ae2a65276c2",
   "metadata": {},
   "source": [
    "### function to generate the UID to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3768e-002e-4cf4-951b-2341f0d5dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uid2index(behaviors_file, output_file=\"uid2index.pkl\"):\n",
    "    \"\"\"\n",
    "    Generate uid2index.pkl mapping user_id to integer indices from behaviors.tsv.\n",
    "    \n",
    "    Args:\n",
    "        behaviors_file (str): Path to the behaviors.tsv file.\n",
    "        output_file (str): Path to save the uid2index.pkl file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of user_id to index.\n",
    "    \"\"\"\n",
    "    # Load behaviors.tsv\n",
    "    columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "    behaviors_df = pd.read_csv(behaviors_file, sep=\"\\t\", names=columns)\n",
    "\n",
    "    # Create a mapping of user_id to index\n",
    "    user_ids = behaviors_df[\"user_id\"].dropna().unique()  # Drop NaN and get unique users\n",
    "    uid2index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "\n",
    "    # Save as uid2index.pkl\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(uid2index, f)\n",
    "\n",
    "    print(f\"Created {output_file} with {len(uid2index)} users.\")\n",
    "    return uid2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ca234-de0f-42c7-b54a-623fb5e8ca29",
   "metadata": {},
   "source": [
    "### Function to load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8c33c-f991-4f09-9995-0ad8b6490149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings into a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        glove_file (str): Path to the GloVe file.\n",
    "        embedding_dim (int): Dimension of GloVe vectors.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of words to embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe embeddings\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d14bd-54ed-44d3-b59b-746eba9d8571",
   "metadata": {},
   "source": [
    "### Function to create word_dict and embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72b92f-d2a1-45ea-82d1-b1ea7a232447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dict_and_embeddings(news_file, glove_embeddings, embedding_dim=300, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Create word_dict.pkl and embedding.npy using GloVe embeddings.\n",
    "\n",
    "    Args:\n",
    "        news_file (str): Path to news.tsv.\n",
    "        glove_embeddings (dict): Loaded GloVe embeddings.\n",
    "        embedding_dim (int): Dimension of GloVe vectors.\n",
    "        output_dir (str): Directory to save outputs.\n",
    "    \n",
    "    Returns:\n",
    "        dict, np.ndarray: word_dict and embedding_matrix.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Load news data\n",
    "    news_df = pd.read_csv(news_file, sep=\"\\t\", names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "\n",
    "    # Tokenize titles and abstracts\n",
    "    def tokenize(text):\n",
    "        return word_tokenize(text.lower())  # Use the recommenders utility function\n",
    "\n",
    "    all_text = news_df[\"title\"].fillna(\"\") + \" \" + news_df[\"abstract\"].fillna(\"\")\n",
    "    tokens = []\n",
    "    for text in all_text:\n",
    "        tokens.extend(tokenize(text))\n",
    "    \n",
    "    # Count word frequencies and create word_dict\n",
    "    word_counter = Counter(tokens)\n",
    "    word_dict = {word: idx for idx, (word, _) in enumerate(word_counter.items(), start=1)}  # Start index at 1\n",
    "    \n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embedding_dim))  # Extra row for padding (index 0)\n",
    "    for word, idx in word_dict.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random vector for unknown words\n",
    "    \n",
    "    # Save word_dict and embedding matrix\n",
    "    word_dict_file = os.path.join(output_dir, \"word_dict.pkl\")\n",
    "    embedding_file = os.path.join(output_dir, \"embedding.npy\")\n",
    "    with open(word_dict_file, \"wb\") as f:\n",
    "        pickle.dump(word_dict, f)\n",
    "    np.save(embedding_file, embedding_matrix)\n",
    "\n",
    "    print(f\"Saved word_dict.pkl and embedding.npy to {output_dir}.\")\n",
    "    return word_dict, embedding_matrix\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59afa67-e97c-4fff-be7f-723471666196",
   "metadata": {},
   "source": [
    "## Identify the news and behavior files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e4d86-dacf-4981-8da7-b84c38181c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd3c9b-53e4-431f-9762-5fed967ba323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uid2index.pkl\n",
    "uid2index = generate_uid2index(train_behaviors_file, output_file=\"/app/datasets/suber_stuff/uid2index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554c70d-07cd-4f5e-87b9-685dbc1aabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to GloVe file (e.g., glove.6B.300d.txt)\n",
    "glove_file = os.path.join(glove_dir, \"glove/glove.6B.300d.txt\")\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_file, embedding_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a1c71-3e93-4b53-b00f-d63c1330b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word_dict.pkl and embedding.npy\n",
    "word_dict, embedding_matrix = create_word_dict_and_embeddings(\n",
    "    news_file= train_news_file,\n",
    "    glove_embeddings=glove_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dir=\"/app/datasets/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95affeba-a5ff-4c52-9dc3-0c5ad23153c3",
   "metadata": {},
   "source": [
    "## Define the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dba2ff-4428-47a5-9cb7-298d8def9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(None, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          model_type=\"nrms\",\n",
    "                          title_size=30,\n",
    "                          his_size=50,\n",
    "                          npratio=4,\n",
    "                          data_format='news',\n",
    "                          word_emb_dim=300,\n",
    "                          head_num=20,\n",
    "                          head_dim=20,\n",
    "                          attention_hidden_dim=200,\n",
    "                          loss='cross_entropy_loss',\n",
    "                          dropout=0.2,\n",
    "                          support_quick_scoring=True,\n",
    "                          show_step=10)\n",
    "#print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d083bd0-20ca-4356-b9e4-15c9d08d1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hparams.values().keys():\n",
    "    print(f\"{i} = {hparams.values()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093f31e-49ad-4ceb-87f0-586681c0f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "iterator = MINDIterator\n",
    "model = NRMSModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149ab9f-3527-430f-993f-ae93c557956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18901dab-1155-4a46-a654-baf24cd6495d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9eec7-a401-4302-84c9-3bed9b9c7654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410d89d-8fc6-411c-a9e8-eaa7fc454d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sb3)",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
