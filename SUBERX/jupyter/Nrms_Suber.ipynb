{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57f4dc5-e2d4-48af-b634-a36f1b0ba6bc",
   "metadata": {},
   "source": [
    "# CICERO -- SUBER with NRMS as the model to be trained\n",
    "\n",
    "How can we take the NRMS model and use it as input for SUBER?  By that I mean, how it provides the data in the form that the original NRMS model uses \n",
    "\n",
    "The NRMS  data comes out of the recommenders module itself.  After one does a pip install recommenders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc102976-1a42-45a9-b451-a81e5341ead2",
   "metadata": {},
   "source": [
    "## Add the data from NRMS first using the recommenders module\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff884381-7536-4fcf-ad55-c1ba57a1204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
      "Tensorflow version: 2.15.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove warnings\n",
    "import os\n",
    "os.environ['TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1d3be2-8447-437f-835a-768fc38d063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.1\n",
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU is available and TensorFlow is using it.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# List available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Check if a GPU is detected\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. TensorFlow is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af570-5c83-4b45-bd09-41ccf60f9bea",
   "metadata": {},
   "source": [
    "## Prepare Parameters (will need to do something else for SUBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5974d7-912e-4bff-b82e-6eb54aef5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "# Options: demo, small, large\n",
    "# My modification for MINDSMALL as I mount in the data into the container.  Further the small dataset is not accessable anymore.\n",
    "\n",
    "\n",
    "MIND_type = 'MINDsmall'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b31c52-eecd-446e-a167-1bfce7933a55",
   "metadata": {},
   "source": [
    "## Download and load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29954d8-37fb-422d-9b3f-fc874fe01510",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "data_path = \"/app/SUBERX/datasets/\" + MIND_type\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path is {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610d53a-c6a9-44e0-a82f-3b38838c3b0f",
   "metadata": {},
   "source": [
    "## Download Glove embeddings\n",
    "\n",
    "The original NRMS used glove embeddings so will we."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9bf3e-275d-4371-9776-639ce23b1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.mind import download_and_extract_glove\n",
    "\n",
    "# Download and extract GloVe embeddings\n",
    "glove_dir = '/app/SUBERX/glove_embeddings'\n",
    "download_and_extract_glove(glove_dir)\n",
    "print(f\"GloVe embeddings extracted to: {glove_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7569bae-6674-4fa8-9c33-4ae2a65276c2",
   "metadata": {},
   "source": [
    "### function to generate the UID to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3768e-002e-4cf4-951b-2341f0d5dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uid2index(behaviors_file, output_file=\"uid2index.pkl\"):\n",
    "    \"\"\"\n",
    "    Generate uid2index.pkl mapping user_id to integer indices from behaviors.tsv.\n",
    "    \n",
    "    Args:\n",
    "        behaviors_file (str): Path to the behaviors.tsv file.\n",
    "        output_file (str): Path to save the uid2index.pkl file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of user_id to index.\n",
    "    \"\"\"\n",
    "    # Load behaviors.tsv\n",
    "    columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "    behaviors_df = pd.read_csv(behaviors_file, sep=\"\\t\", names=columns)\n",
    "\n",
    "    # Create a mapping of user_id to index\n",
    "    user_ids = behaviors_df[\"user_id\"].dropna().unique()  # Drop NaN and get unique users\n",
    "    uid2index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "\n",
    "    # Save as uid2index.pkl\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(uid2index, f)\n",
    "\n",
    "    print(f\"Created {output_file} with {len(uid2index)} users.\")\n",
    "    return uid2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670bf07-0953-427e-aefa-ee6698cc6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8c33c-f991-4f09-9995-0ad8b6490149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings into a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        glove_file (str): Path to the GloVe file.\n",
    "        embedding_dim (int): Dimension of GloVe vectors.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of words to embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe embeddings\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d14bd-54ed-44d3-b59b-746eba9d8571",
   "metadata": {},
   "source": [
    "### Function to create word_dict and embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72b92f-d2a1-45ea-82d1-b1ea7a232447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dict_and_embeddings(news_file, glove_embeddings, embedding_dim=300, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Create word_dict.pkl and embedding.npy using GloVe embeddings.\n",
    "\n",
    "    Args:\n",
    "        news_file (str): Path to news.tsv.\n",
    "        glove_embeddings (dict): Loaded GloVe embeddings.\n",
    "        embedding_dim (int): Dimension of GloVe vectors.\n",
    "        output_dir (str): Directory to save outputs.\n",
    "    \n",
    "    Returns:\n",
    "        dict, np.ndarray: word_dict and embedding_matrix.\n",
    "    \"\"\"\n",
    "    # Load news data\n",
    "    news_df = pd.read_csv(news_file, sep=\"\\t\", names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "\n",
    "    # Tokenize titles and abstracts\n",
    "    def tokenize(text):\n",
    "        return word_tokenize(text.lower())  # Use the recommenders utility function\n",
    "\n",
    "    all_text = news_df[\"title\"].fillna(\"\") + \" \" + news_df[\"abstract\"].fillna(\"\")\n",
    "    tokens = []\n",
    "    for text in all_text:\n",
    "        tokens.extend(tokenize(text))\n",
    "    \n",
    "    # Count word frequencies and create word_dict\n",
    "    word_counter = Counter(tokens)\n",
    "    word_dict = {word: idx for idx, (word, _) in enumerate(word_counter.items(), start=1)}  # Start index at 1\n",
    "    \n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embedding_dim))  # Extra row for padding (index 0)\n",
    "    for word, idx in word_dict.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random vector for unknown words\n",
    "    \n",
    "    # Save word_dict and embedding matrix\n",
    "    word_dict_file = os.path.join(output_dir, \"word_dict.pkl\")\n",
    "    embedding_file = os.path.join(output_dir, \"embedding.npy\")\n",
    "    with open(word_dict_file, \"wb\") as f:\n",
    "        pickle.dump(word_dict, f)\n",
    "    np.save(embedding_file, embedding_matrix)\n",
    "\n",
    "    print(f\"Saved word_dict.pkl and embedding.npy to {output_dir}.\")\n",
    "    return word_dict, embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59afa67-e97c-4fff-be7f-723471666196",
   "metadata": {},
   "source": [
    "## Identify the news and behavior files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e4d86-dacf-4981-8da7-b84c38181c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd3c9b-53e4-431f-9762-5fed967ba323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uid2index.pkl\n",
    "uid2index = generate_uid2index(behaviors_file, output_file=\"uid2index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554c70d-07cd-4f5e-87b9-685dbc1aabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to GloVe file (e.g., glove.6B.300d.txt)\n",
    "glove_file = os.path.join(glove_dir, \"glove/glove.6B.300d.txt\")\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_file, embedding_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a1c71-3e93-4b53-b00f-d63c1330b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word_dict.pkl and embedding.npy\n",
    "word_dict, embedding_matrix = create_word_dict_and_embeddings(\n",
    "    news_file=\"path/to/news.tsv\",\n",
    "    glove_embeddings=glove_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dir=\".\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954de25-1bf2-4315-aad2-f861ec8fe595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce84097-4700-49e4-8c81-84c2f8d66483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'nrms.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
    "    \n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url, \\\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/', \\\n",
    "                               os.path.join(data_path, 'utils'), mind_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51976a9-fa91-45a9-83f7-b66f9973922c",
   "metadata": {},
   "source": [
    "## Train the NRMS Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093f31e-49ad-4ceb-87f0-586681c0f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86716879-3990-49d9-9c1e-0f5d4126fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56042623-3609-41a1-aea3-0c54c468affe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f3596-f2c1-46b9-8a28-339e41553a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7195329-5909-4a5a-8bae-94b00bd83135",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cc0c3-6a3b-491b-8853-0b9ecf022b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env |grep PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d89faa-8641-491d-8e76-95c4502cb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env |grep LD_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149ab9f-3527-430f-993f-ae93c557956d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sb3)",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
