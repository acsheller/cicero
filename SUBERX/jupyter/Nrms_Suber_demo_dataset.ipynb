{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49d0a1a-f8a3-4816-802e-4580a1673cd0",
   "metadata": {},
   "source": [
    "# CICERO -- NRMS Demo Dataset Adapted\n",
    "\n",
    "This notebook is a run of the [NRMS notebook]( adapted from the [Recommenders Team](https://github.com/recommenders-team/recommenders/tree/main).  It has been modified as time has caused some things to not work as presneted originally. The original `demo` did work, however it did not work for the `small` or `large` datasets, those needed extra work.\n",
    "\n",
    "NRMS stands for `Neural News Reccomendaiton with Multi-head Self-Attention`.  The reference to the paper is provided below. please look over and study the [recommenders team github repository](https://github.com/recommenders-team/recommenders/blob/main/README.md#Getting-Started) because this code is derived from it. \n",
    "\n",
    "Read up on the [MIND: MIcrosoft News Dataset](https://msnews.github.io/) and download the data into the datasets folder of this project and place them in `/apps/datasets`.  Review the [Readme](/app/datasets/README.md) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed8aee-0daa-442f-8b06-f244e871f4e7",
   "metadata": {},
   "source": [
    "## Notebook and GPU Check.  \n",
    "\n",
    "Run the follow-on cell to check things out.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5238d5c-9957-45d8-9028-16519f4ab63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
      "Tensorflow version: 2.15.1\n",
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU is available and TensorFlow is using it.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove warnings\n",
    "import os\n",
    "os.environ['TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# List available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Check if a GPU is detected\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. TensorFlow is using the CPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371cf5a-f9de-48cb-89d4-852da56d7c61",
   "metadata": {},
   "source": [
    "## Prepare Parameters\n",
    "\n",
    "adjust these as needed but this is fine for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d4cd28-38c9-44f6-8c65-54f1f6aecb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "# Options: MINDdemo, MINDsmall, MINDlarge\n",
    "\n",
    "#\n",
    "MIND_type = 'MINDdemo'\n",
    "data_path_base = \"/app/datasets/\"\n",
    "data_path = \"/app/datasets/\" + MIND_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6f45f-d7b2-43b7-ab2f-ae899c5429d5",
   "metadata": {},
   "source": [
    "## Download and Load the Data\n",
    "\n",
    "The [original, NRMS MIND notebook](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb) was crafted back in 2021 and the links for the data are inaccesable. Download from the MIND dataset website -- both the small and large datasets. \n",
    "\n",
    "While the `demo` dataset can be used the `small` and `large` datasets are unavailable.  The Notebook was modified to use a local copy of the datasets and supporting files.  Note there are several files in addition to the data files that are necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad33f2-186a-46a3-b9c5-85a7e60538e7",
   "metadata": {},
   "source": [
    "## Data Paths\n",
    "\n",
    "The original NRMS MIDN did alot of things for you.  the utils folder contains those things that it did. Remember that we have downloaded the data ahead of time for this.  The way I got the pkl and npy files is by executing hte original nrmsMIND notebook with demo set. In case you're wondering.\n",
    "\n",
    "Glove embeddings and nltk are not provided here as the `demo` dataset downloads themselves.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884c3e98-ab8e-4857-8870-a6f95421d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path is /app/datasets/MINDdemo\n"
     ]
    }
   ],
   "source": [
    "# Create the directory if it doesn't exist but it will\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path is {data_path}\")\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'nrms.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da513a4b-6c55-4b5a-9536-ba045a7d5012",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927bda0a-22c7-42d9-8911-fe5eb1c1d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 20, 'head_dim': 20, 'filter_num': 200, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 5, 'batch_size': 64, 'show_step': 10, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'model_type': 'nrms', 'loss': 'cross_entropy_loss', 'wordEmb_file': '/app/datasets/MINDdemo/utils/embedding.npy', 'wordDict_file': '/app/datasets/MINDdemo/utils/word_dict.pkl', 'userDict_file': '/app/datasets/MINDdemo/utils/uid2index.pkl'}\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          show_step=10)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114db8bd-3bb1-4a19-8092-5f651a00ab6f",
   "metadata": {},
   "source": [
    "## Train the NRMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92734de-334f-4cdc-b2f5-b89661db4209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support_quick_scoring = True\n",
      "dropout = 0.2\n",
      "attention_hidden_dim = 200\n",
      "head_num = 20\n",
      "head_dim = 20\n",
      "filter_num = 200\n",
      "window_size = 3\n",
      "vert_emb_dim = 100\n",
      "subvert_emb_dim = 100\n",
      "gru_unit = 400\n",
      "type = ini\n",
      "user_emb_dim = 50\n",
      "learning_rate = 0.0001\n",
      "optimizer = adam\n",
      "epochs = 5\n",
      "batch_size = 64\n",
      "show_step = 10\n",
      "title_size = 30\n",
      "his_size = 50\n",
      "data_format = news\n",
      "npratio = 4\n",
      "metrics = ['group_auc', 'mean_mrr', 'ndcg@5;10']\n",
      "word_emb_dim = 300\n",
      "model_type = nrms\n",
      "loss = cross_entropy_loss\n",
      "wordEmb_file = /app/datasets/MINDdemo/utils/embedding.npy\n",
      "wordDict_file = /app/datasets/MINDdemo/utils/word_dict.pkl\n",
      "userDict_file = /app/datasets/MINDdemo/utils/uid2index.pkl\n"
     ]
    }
   ],
   "source": [
    "for i in hparams.values().keys():\n",
    "    print(f\"{i} = {hparams.values()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97f638a-7c99-4a85-af77-aa0bd1000729",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6507fab-a26f-4430-80c3-ed7cc9a277d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = NRMSModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effddac0-6d35-4d4f-88d4-a7b53548229d",
   "metadata": {},
   "source": [
    "### Run the model without any training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e17bad1-e318-4d09-8ff9-5bba852be4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "293it [00:01, 192.41it/s]\n",
      "118it [00:05, 21.62it/s]\n",
      "7538it [00:01, 4862.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.4794, 'mean_mrr': 0.2059, 'ndcg@5': 0.205, 'ndcg@10': 0.2701}\n"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(valid_news_file, valid_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d4244e-4dee-4ee0-a34d-ea65ecef0945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 540 , total_loss: 1.5332, data_loss: 1.4426: : 543it [01:44,  5.19it/s]\n",
      "293it [00:00, 414.01it/s]\n",
      "118it [00:04, 23.79it/s]\n",
      "7538it [00:01, 6336.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.5327468965593622\n",
      "eval info: group_auc:0.5636, mean_mrr:0.2342, ndcg@10:0.3196, ndcg@5:0.2477\n",
      "at epoch 1 , train time: 104.6 eval time: 16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 540 , total_loss: 1.4415, data_loss: 1.3705: : 543it [01:40,  5.42it/s]\n",
      "293it [00:00, 434.91it/s]\n",
      "118it [00:05, 23.22it/s]\n",
      "7538it [00:01, 5406.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.4417780218422962\n",
      "eval info: group_auc:0.5948, mean_mrr:0.2492, ndcg@10:0.3389, ndcg@5:0.2666\n",
      "at epoch 2 , train time: 100.3 eval time: 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 540 , total_loss: 1.4036, data_loss: 1.2445: : 543it [01:39,  5.43it/s]\n",
      "293it [00:00, 431.04it/s]\n",
      "118it [00:04, 27.60it/s]\n",
      "7538it [00:01, 4592.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.4037476183081639\n",
      "eval info: group_auc:0.6025, mean_mrr:0.2592, ndcg@10:0.3503, ndcg@5:0.2784\n",
      "at epoch 3 , train time: 99.9 eval time: 16.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 540 , total_loss: 1.3750, data_loss: 1.2596: : 543it [01:40,  5.42it/s]\n",
      "293it [00:00, 429.67it/s]\n",
      "118it [00:05, 23.50it/s]\n",
      "7538it [00:01, 4395.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.3751572317839986\n",
      "eval info: group_auc:0.6008, mean_mrr:0.26, ndcg@10:0.3504, ndcg@5:0.2781\n",
      "at epoch 4 , train time: 100.2 eval time: 16.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 540 , total_loss: 1.3535, data_loss: 1.4076: : 543it [01:41,  5.34it/s]\n",
      "293it [00:00, 377.28it/s]\n",
      "118it [00:05, 23.02it/s]\n",
      "7538it [00:04, 1523.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 5\n",
      "train info: logloss loss:1.3536139331469879\n",
      "eval info: group_auc:0.6055, mean_mrr:0.2637, ndcg@10:0.3556, ndcg@5:0.2824\n",
      "at epoch 5 , train time: 101.7 eval time: 20.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommenders.models.newsrec.models.nrms.NRMSModel at 0x7fc9a483e9e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c9e9383-77e6-4036-a323-8fd31c26b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:10:17\n"
     ]
    }
   ],
   "source": [
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Convert the elapsed time into hours, minutes, and seconds\n",
    "hours, remainder = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "# Print the result in H:M:S format\n",
    "print(f\"Elapsed time: {int(hours)}:{int(minutes)}:{int(seconds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb622dff-5e9a-4646-ada1-f64286d775db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "293it [00:00, -1069.05it/s]\n",
      "118it [00:05, 23.26it/s]\n",
      "7538it [00:04, 1560.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.6055, 'mean_mrr': 0.2637, 'ndcg@5': 0.2824, 'ndcg@10': 0.3556}\n",
      "CPU times: user 30.9 s, sys: 1min 40s, total: 2min 11s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cfb1ea4-b13b-479e-9568-fb130ce8384b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.6055,
       "encoder": "json",
       "name": "group_auc"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "group_auc"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.2637,
       "encoder": "json",
       "name": "mean_mrr"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "mean_mrr"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.2824,
       "encoder": "json",
       "name": "ndcg@5"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "ndcg@5"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.3556,
       "encoder": "json",
       "name": "ndcg@10"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "ndcg@10"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record results for tests - ignore this cell\n",
    "store_metadata(\"group_auc\", res_syn['group_auc'])\n",
    "store_metadata(\"mean_mrr\", res_syn['mean_mrr'])\n",
    "store_metadata(\"ndcg@5\", res_syn['ndcg@5'])\n",
    "store_metadata(\"ndcg@10\", res_syn['ndcg@10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84e72d-da82-44b2-b417-e51ae31d6e59",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aff3bd9-f688-4e1a-a67b-28b464f395a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4dc560-35a0-4480-8fd5-a6336741c404",
   "metadata": {},
   "source": [
    "## Output Prediction File\n",
    "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
    "\n",
    "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02016c79-9beb-4111-bb4b-cceb58e6d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "293it [00:01, 263.97it/s]\n",
      "118it [00:05, 23.23it/s]\n",
      "7538it [00:04, 1773.38it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53481d6-807e-4222-9c9e-caa96caf551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7538it [00:00, 49783.75it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db67bac7-860a-4699-8fc6-a9f8bffda7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b3378-bc93-413b-8cab-ea9581a73938",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\n",
    "\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2cc39b-cd95-4be9-9520-bfca4eead210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:10:48\n"
     ]
    }
   ],
   "source": [
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Convert the elapsed time into hours, minutes, and seconds\n",
    "hours, remainder = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "# Print the result in H:M:S format\n",
    "print(f\"Elapsed time: {int(hours)}:{int(minutes)}:{int(seconds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bdd9e-fec0-4c32-b2df-16f6c9cbb6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
