{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57f4dc5-e2d4-48af-b634-a36f1b0ba6bc",
   "metadata": {},
   "source": [
    "# CICERO -- NRMS Small Dataset Adapted to SUBER first Steps\n",
    "\n",
    "This notebook is a run of the [NRMS notebook]( adapted from the [Recommenders Team](https://github.com/recommenders-team/recommenders/tree/main).  It has been modified as time has caused some things to not work as presneted originally.\n",
    "\n",
    "NRMS stands for `Neural News Reccomendaiton with Multi-head Self-Attention`.  The reference to the paper is provided below. please look over and study the [recommenders team github repository](https://github.com/recommenders-team/recommenders/blob/main/README.md#Getting-Started) because this code is derived from it. \n",
    "\n",
    "Read up on the [MIND: MIcrosoft News Dataset](https://msnews.github.io/) and download the data into the datasets folder of this project and place them in `/apps/datasets`.  Review the [Readme](/app/datasets/README.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc102976-1a42-45a9-b451-a81e5341ead2",
   "metadata": {},
   "source": [
    "## do imports and check things out.\n",
    "\n",
    "Are we using a GPU? If not then things will take a very long time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff884381-7536-4fcf-ad55-c1ba57a1204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove warnings\n",
    "import os\n",
    "os.environ['TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "# Gets the SUBERX python modules included.\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import zipfile\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "# This module was created to generate the necessary files if they dont' already exist\n",
    "from environment.data_utils import generate_uid2index, load_glove_embeddings, create_word_dict_and_embeddings, setup_nltk_resources\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.datasets.mind import download_and_extract_glove\n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# List available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Check if a GPU is detected\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. TensorFlow is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af570-5c83-4b45-bd09-41ccf60f9bea",
   "metadata": {},
   "source": [
    "## Prepare Parameters\n",
    "\n",
    "Adjust these as needed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5974d7-912e-4bff-b82e-6eb54aef5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# My modification for MINDSMALL as I mount in the data into the container.  Further the small dataset is not accessable anymore.\n",
    "\n",
    "# Options: MINDdemo, MINDsmall, MINDlarge\n",
    "\n",
    "MIND_type = 'MINDsmall'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d489c-d20a-466a-9272-1384108a3032",
   "metadata": {},
   "source": [
    "## Specify the dataset to use\n",
    "\n",
    "There are three in the original notebook: `demo`, `small` and `large`.\n",
    "\n",
    "From the [original  notebook](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb),  the `demo` set is 5000 samples of the `small` dataset.\n",
    "\n",
    "I was able to download the original demo but that's not really needed. A sample of 5000 is good for showing the algorithm works but won't train it well enough.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29954d8-37fb-422d-9b3f-fc874fe01510",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I mount in the datasets folder to /app\n",
    "data_path_base=\"/app/datasets/\"\n",
    "data_path = data_path_base + MIND_type\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path is {data_path}\")\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "dev_news_file = os.path.join(data_path,'dev','news.tsv')\n",
    "dev_behaviors_file = os.path.join(data_path,'dev','behaviors.tsv')\n",
    "\n",
    "wordEmb_file = os.path.join(data_path, 'utils',\"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, 'utils',\"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, 'utils',\"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\",'nrms.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610d53a-c6a9-44e0-a82f-3b38838c3b0f",
   "metadata": {},
   "source": [
    "## Download Glove embeddings\n",
    "\n",
    "The original NRMS used glove embeddings, this is how they are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9bf3e-275d-4371-9776-639ce23b1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All models can use the same glove embeddings\n",
    "\n",
    "# Download and extract GloVe embeddings\n",
    "glove_dir = data_path_base +'glove_embeddings'\n",
    "download_and_extract_glove(glove_dir)\n",
    "print(f\"GloVe embeddings extracted to: {glove_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e611ac-a694-4521-add9-81ec629fc667",
   "metadata": {},
   "source": [
    "## Get the NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f7f76-92e7-4eba-8e8a-b0cb1155c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data_dir = data_path_base + 'nltk_data'\n",
    "setup_nltk_resources(download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59afa67-e97c-4fff-be7f-723471666196",
   "metadata": {},
   "source": [
    "## Identify the news and behavior files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd3c9b-53e4-431f-9762-5fed967ba323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uid2index.pkl\n",
    "uid2index = generate_uid2index(train_behaviors_file, output_file= userDict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554c70d-07cd-4f5e-87b9-685dbc1aabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to GloVe file (e.g., glove.6B.300d.txt)\n",
    "glove_file = os.path.join(glove_dir, \"glove/glove.6B.300d.txt\")\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_file, embedding_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a1c71-3e93-4b53-b00f-d63c1330b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word_dict.pkl and embedding.npy\n",
    "word_dict, embedding_matrix = create_word_dict_and_embeddings(\n",
    "    news_file= train_news_file,\n",
    "    glove_embeddings=glove_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dir= data_path + '/utils/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95affeba-a5ff-4c52-9dc3-0c5ad23153c3",
   "metadata": {},
   "source": [
    "## Define the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dba2ff-4428-47a5-9cb7-298d8def9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(None, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          model_type=\"nrms\",\n",
    "                          title_size=30,\n",
    "                          his_size=50,\n",
    "                          npratio=4,\n",
    "                          data_format='news',\n",
    "                          word_emb_dim=300,\n",
    "                          head_num=20,\n",
    "                          head_dim=20,\n",
    "                          attention_hidden_dim=200,\n",
    "                          loss='cross_entropy_loss',\n",
    "                          dropout=0.2,\n",
    "                          support_quick_scoring=True,\n",
    "                          show_step=10, \n",
    "                          metrics=['group_auc', 'mean_mrr', 'ndcg@5;10'])\n",
    "#print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d083bd0-20ca-4356-b9e4-15c9d08d1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hparams.values().keys():\n",
    "    print(f\"{i} = {hparams.values()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093f31e-49ad-4ceb-87f0-586681c0f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDIterator\n",
    "model = NRMSModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5eb292-d0a2-4fc4-941e-d32a7b39f2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3dc1525-b855-4cc6-bf96-79bea5f21d61",
   "metadata": {},
   "source": [
    "## Figuring out for SUBER Input\n",
    "\n",
    "1. `parsed_data` is a generator.\n",
    "2. `data_d` is a dictionary of one batch of size 64 each.\n",
    "3. Showing the keys shows the 64 batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70851781-ae3a-4991-b368-8cc159cc3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = model.train_iterator.load_data_from_file(train_news_file, train_behaviors_file)\n",
    "print(type(parsed_data))\n",
    "data_d = next(parsed_data)\n",
    "print(type(data_d))\n",
    "\n",
    "for key, value in data_d.items():\n",
    "    print(f\"Key: {key}, Shape: {value.shape if hasattr(value, 'shape') else type(value)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3ea35-4cc3-4f27-a83e-13d296cd7eb4",
   "metadata": {},
   "source": [
    "### Description of keys\n",
    "*Note the first dimension is 64 for all keys.*\n",
    "- `impression_index_batch`(64,1): comes out of the behaviors tsv, one impression id per user in the batch.\n",
    "- `user_index_batch` (64,1): One user id per entry.\n",
    "- `clicked_title_batch`: (64,50,30) Each user has a click history of up to 50 articles. each article title has 30 words.\n",
    "- `candidate_tile_batch` (64,5,30): Each user is shown 5 candidate articles. Each article title has 30 words.\n",
    "-  `labels` (64,5): One label per candidate article foe each user in the batch.This indicates where the user clicked each candidate article (1 for click and 0 for not click).\n",
    "\n",
    "---\n",
    "\n",
    "Next, we run a few opochs for `show and tell` -- just to see that the format of our data is correct for input.  We do this by passing it to `model.train` located in `https://github.com/recommenders-team/recommenders/blob/main/recommenders/models/newsrec/models/base_model.py#L150 `. Take a look at `recommenders/models/newsrec/models/nrms.py` particularly the method `_build_nrms`; this is what \"builds the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b4caf-cb53-43ad-94e0-dd203869d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example that illustrates the shape of the input to NRMS.\n",
    "\n",
    "losses = []\n",
    "for i, batch in enumerate(parsed_data):\n",
    "    loss = model.train(batch)\n",
    "    print(f\"Batch {i + 1} Loss: {loss}\")\n",
    "    losses.append(loss)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab50d1-2a6e-4dcf-9eab-ff02d56a7e93",
   "metadata": {},
   "source": [
    "## Mid-Notebook checkpoint\n",
    "\n",
    "OK, We know how the model will take data in and we know the structure of the data so how can we create it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9409d3b-2678-41a0-a35f-656ead9e534e",
   "metadata": {},
   "source": [
    "# SUBER Integration of NRMS Dataset\n",
    "\n",
    "To integrate the synthtic analysts we need to:\n",
    "1. Do some imports and setup for Pydantic AI\n",
    "2. Open our synthetic users file.\n",
    "3. Generate a UID similar to the ones in `behaviors.tsv`.\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c612516-8970-468e-b818-c12d68a0b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Pydantic AI\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.ollama import OllamaModel\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "base_url = \"http://ollama:11434/v1/\"\n",
    "# Define the Ollama model running on Ollama\n",
    "ollama_model = OllamaModel(\n",
    "    model_name=\"mistral:7b\",  # Replace with your preferred model  Could be 'mistrel:7b', 'granite3.1-dense:latest', 'llama3.2', gemma2\n",
    "    base_url=\"http://ollama:11434/v1/\"  # Ollama's default base URL\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810ffcc-58dd-45be-801d-9a8d3ebf9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_syn_analysts\n",
    "file_path = os.path.join(data_path_base, \"synthetic_analysts.csv\")\n",
    "# Check if the file exists\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    existing_syn_analysts = pd.read_csv(file_path)\n",
    "    print(f\"Existing data loaded. {len(existing_syn_analysts)} records found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebf3d2-d587-49e8-86a5-04a168cd8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create obfuscated IDs\n",
    "num_analysts = len(existing_syn_analysts)\n",
    "obfuscated_ids = [f\"A{random.randint(1000, 9999)}\" for _ in range(num_analysts)]\n",
    "\n",
    "zipped_data = list(zip(obfuscated_ids, existing_syn_analysts[\"name\"]))\n",
    "uid_name_df = pd.DataFrame(zipped_data, columns=[\"uid\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf0aa7-af73-4e0d-a861-c2c707b85b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_name_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f66032-52f9-4533-a63c-34295f8d07a6",
   "metadata": {},
   "source": [
    "## Reminder of the behaviors.tsv columns\n",
    "\n",
    "- `imporession_id`: Basically a unique Identifier for this observation or impression session\n",
    "- `user_id`       : The unique Identifier of the User or Analysts that saw this record\n",
    "- `time`          : data time stamp formatted like: `11/11/2019 11:14:32 AM`\n",
    "- `history`       : a list of articles clicked in the past\n",
    "- `impressions`   : articles shown in this session with a `1` for clicked or a `0` for did not click.  For this session.\n",
    "\n",
    "## Reminder of the news.tsv columns\n",
    "\n",
    "- `news_id`          : A unique ID for each news article\n",
    "- `category`         : The high-level catagory of the news article.\n",
    "- `subcategory`      : More granular classification under the category.\n",
    "- `title`            : The headline of the article.\n",
    "- `abstract`         : A brief summary or preview of the aricle's content.\n",
    "- `url`              : Link to the full article (not useful anymore as these are no long valid URLs -- too old)\n",
    "- `title_entities`   : Structured data indicationg named entities found in the title.\n",
    "- `abstract_entities`: Structured data indicatin named entities found in the abstract.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6835689-362c-4a52-b73c-e96334fa4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the news.tsv file\n",
    "news_df = pd.read_csv(train_news_file, sep=\"\\t\", header=None)\n",
    "\n",
    "# Assign column names based on the standard structure of the news.tsv file\n",
    "news_df.columns = [\n",
    "    \"news_id\", \n",
    "    \"category\", \n",
    "    \"subcategory\", \n",
    "    \"title\", \n",
    "    \"abstract\", \n",
    "    \"url\", \n",
    "    \"title_entities\", \n",
    "    \"abstract_entities\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2e7eb-1d90-438c-852e-88e3fbd5ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db145a-6443-4c80-a690-b67f07da150d",
   "metadata": {},
   "source": [
    "## Define a behavior class for Synthetic Users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e49b1-228b-4dd9-97a9-7ae06b3d1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AnalystBehavior(BaseModel):\n",
    "    \"\"\"\n",
    "    This is the structure the LLM will return for behaviors.\n",
    "    \"\"\"\n",
    "    impression_id: str = Field(description=\"A unique session identifier for this behavior entry.\")\n",
    "    user_id: str = Field(description=\"The unique identifier of the analyst.\")\n",
    "    time: str = Field(description=\"Timestamp of the session in MM/DD/YYYY HH:MM:SS AM/PM format.\")\n",
    "    history: str = Field(description=\"A string of previously clicked article IDs, separated by spaces. Only add the newsID if the article was selected.\")\n",
    "    impressions: str = Field(description=(\n",
    "        \"A string of article IDs separated by spaces, where each ID is followed by '-1' if clicked \"\n",
    "        \"or '-0' if not clicked (e.g., 'N12345-1 N23456-0').\"\n",
    "    ))\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"AnalystBehavior:\\n\"\n",
    "            f\"  Impression ID: {self.impression_id}\\n\"\n",
    "            f\"  User ID: {self.user_id}\\n\"\n",
    "            f\"  Time: {self.time}\\n\"\n",
    "            f\"  History: {self.history}\\n\"\n",
    "            f\"  Impressions: {self.impressions}\\n\"\n",
    "        )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee824946-9abb-424e-b58d-15625374a16b",
   "metadata": {},
   "source": [
    "## Define an Agent First\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956f27b-015f-4dfa-9036-c5d4f8050887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = Agent(model=ollama_model, result_type=AnalystBehavior, retries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd9b41-1c2c-4f76-80b4-f7450a95c249",
   "metadata": {},
   "source": [
    "## Let's regroup again\n",
    "\n",
    "We have\n",
    "\n",
    "1. uid_name_df: The UserID to Name Index\n",
    "2. existing_syn_analyst\n",
    "3. train_news_df\n",
    "\n",
    "And we need these arguments to our function for generating AnalystBehaviors:\n",
    "\n",
    "1. An Agent\n",
    "2. An Analyst\n",
    "3. Histories\n",
    "4. Impresisons\n",
    "5. retries\n",
    "6. delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f854727-7f9c-4588-a443-099cab433863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on the \"name\" column\n",
    "merged_df = uid_name_df.merge(existing_syn_analysts, on=\"name\", how=\"inner\")\n",
    "# Convert each analyst row into a dictionary\n",
    "analyst_dicts = merged_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d6014-6980-4af0-969e-8bd75d4d2ceb",
   "metadata": {},
   "source": [
    "## Create a History DataFrame and functions. \n",
    "\n",
    "This will be saved in datasets -- keep that in mind. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb393eb-823d-4c4f-a464-cbe403f0342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the history DataFrame\n",
    "history_df = pd.DataFrame({\n",
    "    \"uid\": merged_df[\"uid\"],  # Take UIDs from the merged DataFrame\n",
    "    \"history\": [[] for _ in range(len(merged_df))]  # Start with empty lists\n",
    "})\n",
    "\n",
    "# Inspect the initial history DataFrame\n",
    "print(\"Initial History DataFrame:\")\n",
    "display(history_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100dfb2-6a8f-4e71-bcfb-6462ecef4391",
   "metadata": {},
   "source": [
    "### Function to update and get the history for the UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f137b-ccbc-418c-ae86-c28687950307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(uid, new_articles):\n",
    "    \"\"\"\n",
    "    Append new articles to an analyst's history.\n",
    "    \n",
    "    Args:\n",
    "        uid (str): The UID of the analyst.\n",
    "        new_articles (list): List of new article IDs to add to history.\n",
    "    \"\"\"\n",
    "    global history_df\n",
    "    # Locate the analyst's row and update the history\n",
    "    idx = history_df.index[history_df[\"uid\"] == uid]\n",
    "    if not idx.empty:\n",
    "        history_df.at[idx[0], \"history\"] += new_articles\n",
    "    else:\n",
    "        print(f\"UID {uid} not found in history_df.\")\n",
    "\n",
    "def get_history(uid):\n",
    "    \"\"\"\n",
    "    Retrieve the history for a specific UID.\n",
    "    \n",
    "    Args:\n",
    "        uid (str): The UID of the analyst.\n",
    "    \n",
    "    Returns:\n",
    "        list: The history of article IDs.\n",
    "    \"\"\"\n",
    "    row = history_df.loc[history_df[\"uid\"] == uid]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0][\"history\"]\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c434b01-8ca9-4602-82c9-bef1b0be258b",
   "metadata": {},
   "source": [
    "## Impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a107bf-e9c7-4566-8596-6376f0e959f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty impressions DataFrame\n",
    "impressions_df = pd.DataFrame(columns=[\"impression_id\", \"uid\", \"news_id\", \"clicked\"])\n",
    "\n",
    "# Inspect the empty DataFrame\n",
    "#print(\"Initial Impressions DataFrame:\")\n",
    "#print(impressions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048c631-9385-4442-81ec-327b2ee7b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_impressions(impression_id, uid, articles, clicks):\n",
    "    \"\"\"\n",
    "    Add impressions for a specific session to the impressions DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        impression_id (str): Unique ID for the session.\n",
    "        uid (str): Analyst's unique ID.\n",
    "        articles (list): List of news article IDs shown in the session.\n",
    "        clicks (list): List of binary values indicating if each article was clicked (1) or not clicked (0).\n",
    "    \"\"\"\n",
    "    global impressions_df\n",
    "    new_impressions = pd.DataFrame({\n",
    "        \"impression_id\": [impression_id] * len(articles),\n",
    "        \"uid\": [uid] * len(articles),\n",
    "        \"news_id\": articles,\n",
    "        \"clicked\": clicks\n",
    "    })\n",
    "    impressions_df = pd.concat([impressions_df, new_impressions], ignore_index=True)\n",
    "\n",
    "# Example: Add impressions for a session\n",
    "#add_impressions(\"IMP001\", \"A1234\", [\"N1\", \"N2\", \"N3\"], [1, 0, 1])\n",
    "\n",
    "def get_impressions(impression_id=None, uid=None):\n",
    "    \"\"\"\n",
    "    Retrieve impressions for a specific session or analyst.\n",
    "    \n",
    "    Args:\n",
    "        impression_id (str): The session ID to filter by (optional).\n",
    "        uid (str): The UID of the analyst to filter by (optional).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame of impressions.\n",
    "    \"\"\"\n",
    "    if impression_id:\n",
    "        return impressions_df[impressions_df[\"impression_id\"] == impression_id]\n",
    "    elif uid:\n",
    "        return impressions_df[impressions_df[\"uid\"] == uid]\n",
    "    else:\n",
    "        return impressions_df  # Return all impressions if no filters are applied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766df05-a1e9-4a6f-b490-ff52c175025a",
   "metadata": {},
   "source": [
    "## 1. Select a Session\n",
    "\n",
    "Sessions are what is shown to the analyst. Behaviors are what the analyst did with what was shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edcf11-b9e9-4b23-a268-3ed65b646d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_articles_for_session(news_df, num_articles=5):\n",
    "    \"\"\"\n",
    "    Select a fixed number of articles randomly from the news DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        news_df (pd.DataFrame): The DataFrame containing news articles.\n",
    "        num_articles (int): The number of articles to select for the session.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of selected articles for the session.\n",
    "    \"\"\"\n",
    "    return news_df.sample(n=num_articles)\n",
    "\n",
    "# Example usage: Select 5 articles\n",
    "#session_articles = select_articles_for_session(news_df, num_articles=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b0a86-2cdb-40cd-bf36-48ae7dada3af",
   "metadata": {},
   "source": [
    "## An example for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be442a50-ccb9-451f-b76a-923081444401",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_uid = random.choice(history_df[\"uid\"].tolist())\n",
    "selected_uid = 'A3020'\n",
    "# TODO -- may be want to distribute evenly -- make sure it does.\n",
    "analyst = merged_df.loc[merged_df[\"uid\"] == selected_uid].to_dict(orient=\"records\")[0]\n",
    "history = get_history(selected_uid)\n",
    "behaviors_df = pd.read_csv(train_behaviors_file, sep=\"\\t\", header=None)\n",
    "behaviors_df.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "max_impression_id = behaviors_df[\"impression_id\"].max()\n",
    "impression_id = max_impression_id + 1\n",
    "\n",
    "train_news_df = pd.read_csv(train_news_file, sep=\"\\t\", header=None)\n",
    "train_news_df.columns = [\n",
    "    \"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"\n",
    "]\n",
    "\n",
    "\n",
    "session_articles = select_articles_for_session(train_news_df)\n",
    "\n",
    "impressions = session_articles[[\"news_id\",\"title\"]].to_dict(orient=\"records\")\n",
    "\n",
    "for impression in impressions:\n",
    "    impression['impression_id'] = impression_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95955ca-2afa-4358-b99b-afa05a834614",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6835300-58aa-42af-b4f0-419353f89a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class AnalystBehaviorSimulator:\n",
    "    def __init__(self, agent):\n",
    "        \"\"\"\n",
    "        Initialize the simulator with an LLM agent and tracking for analysts.\n",
    "\n",
    "        Args:\n",
    "            agent: The LLM agent configured to generate responses.\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.analyst_data = {}  # Dictionary to store analyst-specific history and impressions\n",
    "        self.history_file = \"/app/datasets/history.tsv\"  # Filepath for user history\n",
    "        self.history_df = self.load_or_create_history()  # Load or create the history file\n",
    "\n",
    "    def load_or_create_history(self):\n",
    "        \"\"\"\n",
    "        Load the user history file if it exists, otherwise create an empty one.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The loaded or newly created history DataFrame.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.history_file):\n",
    "            print(f\"Loading history from {self.history_file}\")\n",
    "            return pd.read_csv(self.history_file, sep=\"\\t\")\n",
    "        else:\n",
    "            print(f\"History file not found. Creating {self.history_file}\")\n",
    "            # Create an empty DataFrame with appropriate columns\n",
    "            df = pd.DataFrame(columns=[\"user_id\", \"history\"])\n",
    "            df.to_csv(self.history_file, sep=\"\\t\", index=False)\n",
    "            return df\n",
    "\n",
    "    def save_history(self):\n",
    "        \"\"\"\n",
    "        Save the current history DataFrame back to the history.tsv file.\n",
    "        \"\"\"\n",
    "        self.history_df.to_csv(self.history_file, sep=\"\\t\", index=False)\n",
    "        print(f\"History saved to {self.history_file}\")\n",
    "\n",
    "    def add_to_history(self, user_id, news_id):\n",
    "        \"\"\"\n",
    "        Add a news article to the user's history.\n",
    "\n",
    "        Args:\n",
    "            user_id (str): The ID of the analyst.\n",
    "            news_id (str): The ID of the news article.\n",
    "        \"\"\"\n",
    "        # Check if the user already has a history entry\n",
    "        if user_id in self.history_df[\"user_id\"].values:\n",
    "            # Append the new article to the existing history\n",
    "            current_history = self.history_df.loc[self.history_df[\"user_id\"] == user_id, \"history\"].values[0]\n",
    "            updated_history = f\"{current_history} {news_id}\" if current_history else news_id\n",
    "            self.history_df.loc[self.history_df[\"user_id\"] == user_id, \"history\"] = updated_history\n",
    "        else:\n",
    "            # Add a new entry for the user\n",
    "            new_entry = {\"user_id\": user_id, \"history\": news_id}\n",
    "            self.history_df = pd.concat([self.history_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "\n",
    "        # Save the updated history\n",
    "        self.save_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1408e6-997c-4569-aba6-f973af07fc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59d147-b2a5-4a1b-aefe-80ffb99cddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dba357-2337-4a63-9d76-15c031cbb771",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ca938-ca9e-4ff4-8eae-b9fe44903799",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_acting_as_person(agent, analyst, history, impression):\n",
    "    gender = {\"F\":'female',\"M\":'male'}\n",
    "    \n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are {analyst['name']}, a {analyst['age']}-year-old {analyst['gender']} working as a {analyst['job']}.\n",
    "    {analyst['description']}\n",
    "\n",
    "    Session Details:\n",
    "    - User ID: {analyst['uid']}\n",
    "    - Impression ID: {impression['impression_id']}\n",
    "\n",
    "    You have previously interacted with the following articles:\n",
    "    {', '.join(history)}\n",
    "\n",
    "    The news article is titled: '{impression['title']}'\n",
    "    - News ID: {impression['news_id']}\n",
    "\n",
    "    Would you click on this article? (Respond with 1 for clicked, 0 for not clicked)\n",
    "    Format this as string of article ID separated by aspace, where the ID is followed by '-1' if clicked \"\n",
    "        \"or '-0' if not clicked (e.g., 'N12345-1 N23456-0').\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Prompt \\n{prompt}\")\n",
    "    # Query the LLM\n",
    "    result = await agent.run(prompt)\n",
    "    print(\"Acting as the Person Response:\")\n",
    "    print(result.data if result else \"No response.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "async def test_acting_on_behalf_of_person(agent, analyst, history, impression):\n",
    "    # Build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert judge of human character and news consumption behavior.\n",
    "\n",
    "    Session Details:\n",
    "    - User ID: {analyst['uid']}\n",
    "    - Impression ID: {impression['impression_id']}\n",
    "\n",
    "    Here is the user's profile:\n",
    "    - Name: {analyst['name']}\n",
    "    - Age: {analyst['age']}\n",
    "    - Gender: {analyst['gender']}\n",
    "    - Primary News Interest: {analyst['primary_news_interest']}\n",
    "    - Secondary News Interest: {analyst['secondary_news_interest']}\n",
    "    - Job: {analyst['job']}\n",
    "    - Description: {analyst['description']}\n",
    "\n",
    "    The user has previously interacted with the following articles:\n",
    "    {', '.join(history)}\n",
    "\n",
    "    Evaluate the following article:\n",
    "    - Title: '{impression['title']}'\n",
    "    - News ID: {impression['news_id']}\n",
    "\n",
    "    Would they click on this article? (Respond with 1 for clicked, 0 for not clicked)\n",
    "    What is your reasoning for clicking or not clicking on the article?\n",
    "    \"\"\"\n",
    "    print(f\"Prompt \\n{prompt}\")\n",
    "    # Query the LLM\n",
    "    result = await agent.run(prompt)\n",
    "    print(\"Acting on Behalf of the Person Response:\")\n",
    "    print(result.data if result else \"No response.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574a5d2-3b8a-4873-8d1e-fad556dd41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both experiments\n",
    "async def run_experiments():\n",
    "    print(\"Experiment 1: Acting as the Person\")\n",
    "    return await test_acting_as_person(agent, analyst, history, impressions[0])\n",
    "\n",
    "    #print(\"\\nExperiment 2: Acting on Behalf of the Person\")\n",
    "    #await test_acting_on_behalf_of_person(agent, analyst, history, impressions[0])\n",
    "\n",
    "# Run the experiments\n",
    "result = await run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59fcb4e-93b4-4d98-8c92-6816b53e35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544accf-b32e-4691-a85e-f7021e6e6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba425003-c361-4c1e-b264-57e9072ad0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.data.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb27781-1457-4b65-8b47-4d3b2ff6ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = result.data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa059d6-0902-4c9a-9da9-0d6754de04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d90e2-204d-4a4c-87c5-45d2b24f66f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe067ae-6835-466c-947b-e1abf3e53caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "230d115b-00ef-4f26-b188-c738006a2baf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149ab9f-3527-430f-993f-ae93c557956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model without any training -- can training do better than no training at all?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18901dab-1155-4a46-a654-baf24cd6495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.run_eval(dev_news_file, dev_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fa858-5dbe-412b-ae3e-999941c93b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_news_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9eec7-a401-4302-84c9-3bed9b9c7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_news_file, train_behaviors_file, dev_news_file, dev_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410d89d-8fc6-411c-a9e8-eaa7fc454d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_syn = model.run_eval(dev_news_file, dev_behaviors_file)\n",
    "print(res_syn)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e61ee-7634-4706-844b-ec2f2b0ec61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record results for tests - ignore this cell\n",
    "store_metadata(\"group_auc\", res_syn['group_auc'])\n",
    "store_metadata(\"mean_mrr\", res_syn['mean_mrr'])\n",
    "store_metadata(\"ndcg@5\", res_syn['ndcg@5'])\n",
    "store_metadata(\"ndcg@10\", res_syn['ndcg@10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a310e45-b582-466e-92b4-dc4aa9c1bcd3",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4ca68-4b8e-4709-8ffe-b9cae7c71c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1f46d-1f22-436f-86eb-621a0fd23a39",
   "metadata": {},
   "source": [
    "## Output Prediction File\n",
    "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
    "\n",
    "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html).\n",
    "\n",
    "This should be some test data.  Only MINDLARGE as it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb15e2-c927-4c22-811e-3f33995326b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(dev_news_file, dev_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd1134-5b08-4118-9220-44eab1c68c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f988a8c-5e8f-4962-bb5e-43a5b6ca3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f9ee5-146e-4c8b-97ad-59c885b46c4a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\n",
    "\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793306ff-fe00-4fff-b7d3-776475b2fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Convert the elapsed time into hours, minutes, and seconds\n",
    "hours, remainder = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "# Print the result in H:M:S format\n",
    "print(f\"Elapsed time: {int(hours)}:{int(minutes)}:{int(seconds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c1739-a12d-438b-b26e-07330ac38514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sb3)",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
