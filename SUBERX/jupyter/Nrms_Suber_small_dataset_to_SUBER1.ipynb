{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57f4dc5-e2d4-48af-b634-a36f1b0ba6bc",
   "metadata": {},
   "source": [
    "# CICERO -- NRMS Small Dataset Adapted to SUBER first Steps\n",
    "\n",
    "This notebook is a run of the [NRMS notebook]( adapted from the [Recommenders Team](https://github.com/recommenders-team/recommenders/tree/main).  It has been modified as time has caused some things to not work as presneted originally.\n",
    "\n",
    "NRMS stands for `Neural News Reccomendaiton with Multi-head Self-Attention`.  The reference to the paper is provided below. please look over and study the [recommenders team github repository](https://github.com/recommenders-team/recommenders/blob/main/README.md#Getting-Started) because this code is derived from it. \n",
    "\n",
    "Read up on the [MIND: MIcrosoft News Dataset](https://msnews.github.io/) and download the data into the datasets folder of this project and place them in `/apps/datasets`.  Review the [Readme](/app/datasets/README.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc102976-1a42-45a9-b451-a81e5341ead2",
   "metadata": {},
   "source": [
    "## do imports and check things out.\n",
    "\n",
    "Are we using a GPU? If not then things will take a very long time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff884381-7536-4fcf-ad55-c1ba57a1204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/app/sb3/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
      "Tensorflow version: 2.15.1\n",
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU is available and TensorFlow is using it.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove warnings\n",
    "import os\n",
    "os.environ['TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "# Gets the SUBERX python modules included.\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import zipfile\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "# This module was created to generate the necessary files if they dont' already exist\n",
    "from environment.data_utils import generate_uid2index, load_glove_embeddings, create_word_dict_and_embeddings, setup_nltk_resources\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.datasets.mind import download_and_extract_glove\n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# List available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Check if a GPU is detected\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. TensorFlow is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af570-5c83-4b45-bd09-41ccf60f9bea",
   "metadata": {},
   "source": [
    "## Prepare Parameters\n",
    "\n",
    "Adjust these as needed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5974d7-912e-4bff-b82e-6eb54aef5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# My modification for MINDSMALL as I mount in the data into the container.  Further the small dataset is not accessable anymore.\n",
    "\n",
    "# Options: MINDdemo, MINDsmall, MINDlarge\n",
    "\n",
    "MIND_type = 'MINDsmall'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d489c-d20a-466a-9272-1384108a3032",
   "metadata": {},
   "source": [
    "## Specify the dataset to use\n",
    "\n",
    "There are three in the original notebook: `demo`, `small` and `large`.\n",
    "\n",
    "From the [original  notebook](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb),  the `demo` set is 5000 samples of the `small` dataset.\n",
    "\n",
    "I was able to download the original demo but that's not really needed. A sample of 5000 is good for showing the algorithm works but won't train it well enough.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29954d8-37fb-422d-9b3f-fc874fe01510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path is /app/datasets/MINDsmall\n"
     ]
    }
   ],
   "source": [
    "## This is how it was done originally\n",
    "#tmpdir = TemporaryDirectory()\n",
    "#data_path = tmpdir.name\n",
    "\n",
    "## I mount in the datasets folder to /app\n",
    "data_path_base=\"/app/datasets/\"\n",
    "data_path = data_path_base + MIND_type\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path is {data_path}\")\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "dev_news_file = os.path.join(data_path,'dev','news.tsv')\n",
    "dev_behaviors_file = os.path.join(data_path,'dev','behaviors.tsv')\n",
    "\n",
    "wordEmb_file = os.path.join(data_path, 'utils',\"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, 'utils',\"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, 'utils',\"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\",'nrms.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610d53a-c6a9-44e0-a82f-3b38838c3b0f",
   "metadata": {},
   "source": [
    "## Download Glove embeddings\n",
    "\n",
    "The original NRMS used glove embeddings, this is how they are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e9bf3e-275d-4371-9776-639ce23b1400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings extracted to: /app/datasets/glove_embeddings\n"
     ]
    }
   ],
   "source": [
    "## All models can use the same glove embeddings\n",
    "\n",
    "# Download and extract GloVe embeddings\n",
    "glove_dir = data_path_base +'glove_embeddings'\n",
    "download_and_extract_glove(glove_dir)\n",
    "print(f\"GloVe embeddings extracted to: {glove_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e611ac-a694-4521-add9-81ec629fc667",
   "metadata": {},
   "source": [
    "## Get the NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418f7f76-92e7-4eba-8e8a-b0cb1155c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /app/datasets/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded to /app/datasets/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /app/datasets/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_data_dir = data_path_base + 'nltk_data'\n",
    "setup_nltk_resources(download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59afa67-e97c-4fff-be7f-723471666196",
   "metadata": {},
   "source": [
    "## Identify the news and behavior files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdbd3c9b-53e4-431f-9762-5fed967ba323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/datasets/MINDsmall/utils/uid2index.pkl already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# Generate uid2index.pkl\n",
    "uid2index = generate_uid2index(train_behaviors_file, output_file= userDict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a554c70d-07cd-4f5e-87b9-685dbc1aabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_embeddings.pkl already exists. Loading embeddings from file.\n"
     ]
    }
   ],
   "source": [
    "# Path to GloVe file (e.g., glove.6B.300d.txt)\n",
    "glove_file = os.path.join(glove_dir, \"glove/glove.6B.300d.txt\")\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_file, embedding_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127a1c71-3e93-4b53-b00f-d63c1330b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist in /app/datasets/MINDsmall/utils/. Loading existing word_dict and embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# Generate word_dict.pkl and embedding.npy\n",
    "word_dict, embedding_matrix = create_word_dict_and_embeddings(\n",
    "    news_file= train_news_file,\n",
    "    glove_embeddings=glove_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dir= data_path + '/utils/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95affeba-a5ff-4c52-9dc3-0c5ad23153c3",
   "metadata": {},
   "source": [
    "## Define the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77dba2ff-4428-47a5-9cb7-298d8def9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(None, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          model_type=\"nrms\",\n",
    "                          title_size=30,\n",
    "                          his_size=50,\n",
    "                          npratio=4,\n",
    "                          data_format='news',\n",
    "                          word_emb_dim=300,\n",
    "                          head_num=20,\n",
    "                          head_dim=20,\n",
    "                          attention_hidden_dim=200,\n",
    "                          loss='cross_entropy_loss',\n",
    "                          dropout=0.2,\n",
    "                          support_quick_scoring=True,\n",
    "                          show_step=10, \n",
    "                          metrics=['group_auc', 'mean_mrr', 'ndcg@5;10'])\n",
    "#print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d083bd0-20ca-4356-b9e4-15c9d08d1da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support_quick_scoring = True\n",
      "dropout = 0.2\n",
      "attention_hidden_dim = 200\n",
      "head_num = 20\n",
      "head_dim = 20\n",
      "filter_num = 200\n",
      "window_size = 3\n",
      "vert_emb_dim = 100\n",
      "subvert_emb_dim = 100\n",
      "gru_unit = 400\n",
      "type = ini\n",
      "user_emb_dim = 50\n",
      "learning_rate = 0.001\n",
      "optimizer = adam\n",
      "epochs = 5\n",
      "batch_size = 64\n",
      "show_step = 10\n",
      "wordEmb_file = /app/datasets/MINDsmall/utils/embedding.npy\n",
      "wordDict_file = /app/datasets/MINDsmall/utils/word_dict.pkl\n",
      "userDict_file = /app/datasets/MINDsmall/utils/uid2index.pkl\n",
      "model_type = nrms\n",
      "title_size = 30\n",
      "his_size = 50\n",
      "npratio = 4\n",
      "data_format = news\n",
      "word_emb_dim = 300\n",
      "loss = cross_entropy_loss\n",
      "metrics = ['group_auc', 'mean_mrr', 'ndcg@5;10']\n"
     ]
    }
   ],
   "source": [
    "for i in hparams.values().keys():\n",
    "    print(f\"{i} = {hparams.values()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4093f31e-49ad-4ceb-87f0-586681c0f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "iterator = MINDIterator\n",
    "model = NRMSModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5eb292-d0a2-4fc4-941e-d32a7b39f2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3dc1525-b855-4cc6-bf96-79bea5f21d61",
   "metadata": {},
   "source": [
    "## Figuring out for SUBER Input\n",
    "\n",
    "1. `parsed_data` is a generator.\n",
    "2. `data_d` is a dictionary of one batch of size 64 each.\n",
    "3. Showing the keys shows the 64 batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70851781-ae3a-4991-b368-8cc159cc3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "<class 'dict'>\n",
      "Key: impression_index_batch, Shape: (64, 1)\n",
      "Key: user_index_batch, Shape: (64, 1)\n",
      "Key: clicked_title_batch, Shape: (64, 50, 30)\n",
      "Key: candidate_title_batch, Shape: (64, 5, 30)\n",
      "Key: labels, Shape: (64, 5)\n"
     ]
    }
   ],
   "source": [
    "parsed_data = model.train_iterator.load_data_from_file(train_news_file, train_behaviors_file)\n",
    "print(type(parsed_data))\n",
    "data_d = next(parsed_data)\n",
    "print(type(data_d))\n",
    "\n",
    "for key, value in data_d.items():\n",
    "    print(f\"Key: {key}, Shape: {value.shape if hasattr(value, 'shape') else type(value)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3ea35-4cc3-4f27-a83e-13d296cd7eb4",
   "metadata": {},
   "source": [
    "### Description of keys\n",
    "*Note the first dimension is 64 for all keys.*\n",
    "- `impression_index_batch`(64,1): comes out of the behaviors tsv, one impression id per user in the batch.\n",
    "- `user_index_batch` (64,1): One user id per entry.\n",
    "- `clicked_title_batch`: (64,50,30) Each user has a click history of up to 50 articles. each article title has 30 words.\n",
    "- `candidate_tile_batch` (64,5,30): Each user is shown 5 candidate articles. Each article title has 30 words.\n",
    "-  `labels` (64,5): One label per candidate article foe each user in the batch.This indicates where the user clicked each candidate article (1 for click and 0 for not click).\n",
    "\n",
    "---\n",
    "\n",
    "Next, we run a few opochs for `show and tell` -- just to see that the format of our data is correct for input.  We do this by passing it to `model.train` located in `https://github.com/recommenders-team/recommenders/blob/main/recommenders/models/newsrec/models/base_model.py#L150 `. Take a look at `recommenders/models/newsrec/models/nrms.py` particularly the method `_build_nrms`; this is what \"builds the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d2b4caf-cb53-43ad-94e0-dd203869d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 Loss: 1.6028367280960083\n",
      "Batch 2 Loss: 1.5509140491485596\n",
      "Batch 3 Loss: 1.6738035678863525\n",
      "Batch 4 Loss: 1.508312463760376\n",
      "Batch 5 Loss: 1.5546257495880127\n",
      "Batch 6 Loss: 1.6112298965454102\n",
      "Batch 7 Loss: 1.6047354936599731\n",
      "Batch 8 Loss: 1.5383238792419434\n",
      "Batch 9 Loss: 1.6257178783416748\n",
      "Batch 10 Loss: 1.663732647895813\n",
      "Batch 11 Loss: 1.5934019088745117\n"
     ]
    }
   ],
   "source": [
    "# An example that illustrates the shape of the input to NRMS.\n",
    "\n",
    "losses = []\n",
    "for i, batch in enumerate(parsed_data):\n",
    "    loss = model.train(batch)\n",
    "    print(f\"Batch {i + 1} Loss: {loss}\")\n",
    "    losses.append(loss)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab50d1-2a6e-4dcf-9eab-ff02d56a7e93",
   "metadata": {},
   "source": [
    "## Mid-Notebook checkpoint\n",
    "\n",
    "OK, We know how the model will take data in and we know the structure of the data so how can we create it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c612516-8970-468e-b818-c12d68a0b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810ffcc-58dd-45be-801d-9a8d3ebf9c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149ab9f-3527-430f-993f-ae93c557956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model without any training -- can training do better than no training at all?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18901dab-1155-4a46-a654-baf24cd6495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.run_eval(dev_news_file, dev_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fa858-5dbe-412b-ae3e-999941c93b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_news_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9eec7-a401-4302-84c9-3bed9b9c7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_news_file, train_behaviors_file, dev_news_file, dev_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410d89d-8fc6-411c-a9e8-eaa7fc454d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_syn = model.run_eval(dev_news_file, dev_behaviors_file)\n",
    "print(res_syn)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e61ee-7634-4706-844b-ec2f2b0ec61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record results for tests - ignore this cell\n",
    "store_metadata(\"group_auc\", res_syn['group_auc'])\n",
    "store_metadata(\"mean_mrr\", res_syn['mean_mrr'])\n",
    "store_metadata(\"ndcg@5\", res_syn['ndcg@5'])\n",
    "store_metadata(\"ndcg@10\", res_syn['ndcg@10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a310e45-b582-466e-92b4-dc4aa9c1bcd3",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4ca68-4b8e-4709-8ffe-b9cae7c71c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1f46d-1f22-436f-86eb-621a0fd23a39",
   "metadata": {},
   "source": [
    "## Output Prediction File\n",
    "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
    "\n",
    "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb15e2-c927-4c22-811e-3f33995326b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd1134-5b08-4118-9220-44eab1c68c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f988a8c-5e8f-4962-bb5e-43a5b6ca3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f9ee5-146e-4c8b-97ad-59c885b46c4a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\n",
    "\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793306ff-fe00-4fff-b7d3-776475b2fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Convert the elapsed time into hours, minutes, and seconds\n",
    "hours, remainder = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "# Print the result in H:M:S format\n",
    "print(f\"Elapsed time: {int(hours)}:{int(minutes)}:{int(seconds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c1739-a12d-438b-b26e-07330ac38514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sb3)",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
