{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57f4dc5-e2d4-48af-b634-a36f1b0ba6bc",
   "metadata": {},
   "source": [
    "# CICERO -- NRMS Small Dataset Adapted\n",
    "\n",
    "This notebook is a run of the [NRMS notebook]( adapted from the [Recommenders Team](https://github.com/recommenders-team/recommenders/tree/main).  It has been modified as time has caused some things to not work as presneted originally.\n",
    "\n",
    "NRMS stands for `Neural News Reccomendaiton with Multi-head Self-Attention`.  The reference to the paper is provided below. please look over and study the [recommenders team github repository](https://github.com/recommenders-team/recommenders/blob/main/README.md#Getting-Started) because this code is derived from it. \n",
    "\n",
    "Read up on the [MIND: MIcrosoft News Dataset](https://msnews.github.io/) and download the data into the datasets folder of this project and place them in `/apps/datasets`.  Review the [Readme](/app/datasets/README.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc102976-1a42-45a9-b451-a81e5341ead2",
   "metadata": {},
   "source": [
    "## do imports and check things out.\n",
    "\n",
    "Are we using a GPU? If not then things will take a very long time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff884381-7536-4fcf-ad55-c1ba57a1204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/app/sb3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n",
      "Tensorflow version: 2.15.1\n",
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU is available and TensorFlow is using it.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove warnings\n",
    "import os\n",
    "os.environ['TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "# Gets the SUBERX python modules included.\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "# This module was created to generate the necessary files if they dont' already exist\n",
    "from environment.data_utils import generate_uid2index, load_glove_embeddings, create_word_dict_and_embeddings, setup_nltk_resources\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.datasets.mind import download_and_extract_glove\n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# List available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Check if a GPU is detected\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. TensorFlow is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af570-5c83-4b45-bd09-41ccf60f9bea",
   "metadata": {},
   "source": [
    "## Prepare Parameters\n",
    "\n",
    "Adjust these as needed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5974d7-912e-4bff-b82e-6eb54aef5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# My modification for MINDSMALL as I mount in the data into the container.  Further the small dataset is not accessable anymore.\n",
    "\n",
    "# Options: MINDdemo, MINDsmall, MINDlarge\n",
    "\n",
    "MIND_type = 'MINDsmall'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d489c-d20a-466a-9272-1384108a3032",
   "metadata": {},
   "source": [
    "## Specify the dataset to use\n",
    "\n",
    "There are three in the original notebook: `demo`, `small` and `large`.\n",
    "\n",
    "From the [original  notebook](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb),  the `demo` set is 5000 samples of the `small` dataset.\n",
    "\n",
    "I was able to download the original demo but that's not really needed. A sample of 5000 is good for showing the algorithm works but won't train it well enough.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29954d8-37fb-422d-9b3f-fc874fe01510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path is /app/datasets/MINDsmall\n"
     ]
    }
   ],
   "source": [
    "## This is how it was done originally\n",
    "#tmpdir = TemporaryDirectory()\n",
    "#data_path = tmpdir.name\n",
    "\n",
    "## I mount in the datasets folder to /app\n",
    "data_path_base=\"/app/datasets/\"\n",
    "data_path = data_path_base + MIND_type\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path is {data_path}\")\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "dev_news_file = os.path.join(data_path,'dev','news.tsv')\n",
    "dev_behaviors_file = os.path.join(data_path,'dev','behaviors.tsv')\n",
    "\n",
    "wordEmb_file = os.path.join(data_path, 'utils',\"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, 'utils',\"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, 'utils',\"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\",'nrms.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610d53a-c6a9-44e0-a82f-3b38838c3b0f",
   "metadata": {},
   "source": [
    "## Download Glove embeddings\n",
    "\n",
    "The original NRMS used glove embeddings, this is how they are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e9bf3e-275d-4371-9776-639ce23b1400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings extracted to: /app/datasets/glove_embeddings\n"
     ]
    }
   ],
   "source": [
    "## All models can use the same glove embeddings\n",
    "\n",
    "# Download and extract GloVe embeddings\n",
    "glove_dir = data_path_base +'glove_embeddings'\n",
    "download_and_extract_glove(glove_dir)\n",
    "print(f\"GloVe embeddings extracted to: {glove_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e611ac-a694-4521-add9-81ec629fc667",
   "metadata": {},
   "source": [
    "## Get the NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418f7f76-92e7-4eba-8e8a-b0cb1155c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /app/datasets/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /app/datasets/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded to /app/datasets/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk_data_dir = data_path_base + 'nltk_data'\n",
    "setup_nltk_resources(download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59afa67-e97c-4fff-be7f-723471666196",
   "metadata": {},
   "source": [
    "## Identify the news and behavior files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdbd3c9b-53e4-431f-9762-5fed967ba323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/datasets/MINDsmall/utils/uid2index.pkl already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# Generate uid2index.pkl\n",
    "uid2index = generate_uid2index(train_behaviors_file, output_file= userDict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a554c70d-07cd-4f5e-87b9-685dbc1aabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings: 400001it [00:23, 17121.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400001 word vectors and saved to glove_embeddings.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Path to GloVe file (e.g., glove.6B.300d.txt)\n",
    "glove_file = os.path.join(glove_dir, \"glove/glove.6B.300d.txt\")\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_file, embedding_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127a1c71-3e93-4b53-b00f-d63c1330b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist in /app/datasets/MINDsmall/utils/. Loading existing word_dict and embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# Generate word_dict.pkl and embedding.npy\n",
    "word_dict, embedding_matrix = create_word_dict_and_embeddings(\n",
    "    news_file= train_news_file,\n",
    "    glove_embeddings=glove_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dir= data_path + '/utils/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95affeba-a5ff-4c52-9dc3-0c5ad23153c3",
   "metadata": {},
   "source": [
    "## Define the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77dba2ff-4428-47a5-9cb7-298d8def9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(None, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          model_type=\"nrms\",\n",
    "                          title_size=30,\n",
    "                          his_size=50,\n",
    "                          npratio=4,\n",
    "                          data_format='news',\n",
    "                          word_emb_dim=300,\n",
    "                          head_num=20,\n",
    "                          head_dim=20,\n",
    "                          attention_hidden_dim=200,\n",
    "                          loss='cross_entropy_loss',\n",
    "                          dropout=0.2,\n",
    "                          support_quick_scoring=True,\n",
    "                          show_step=10, \n",
    "                          metrics=['group_auc', 'mean_mrr', 'ndcg@5;10'])\n",
    "#print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d083bd0-20ca-4356-b9e4-15c9d08d1da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support_quick_scoring = True\n",
      "dropout = 0.2\n",
      "attention_hidden_dim = 200\n",
      "head_num = 20\n",
      "head_dim = 20\n",
      "filter_num = 200\n",
      "window_size = 3\n",
      "vert_emb_dim = 100\n",
      "subvert_emb_dim = 100\n",
      "gru_unit = 400\n",
      "type = ini\n",
      "user_emb_dim = 50\n",
      "learning_rate = 0.001\n",
      "optimizer = adam\n",
      "epochs = 5\n",
      "batch_size = 64\n",
      "show_step = 10\n",
      "wordEmb_file = /app/datasets/MINDsmall/utils/embedding.npy\n",
      "wordDict_file = /app/datasets/MINDsmall/utils/word_dict.pkl\n",
      "userDict_file = /app/datasets/MINDsmall/utils/uid2index.pkl\n",
      "model_type = nrms\n",
      "title_size = 30\n",
      "his_size = 50\n",
      "npratio = 4\n",
      "data_format = news\n",
      "word_emb_dim = 300\n",
      "loss = cross_entropy_loss\n",
      "metrics = ['group_auc', 'mean_mrr', 'ndcg@5;10']\n"
     ]
    }
   ],
   "source": [
    "for i in hparams.values().keys():\n",
    "    print(f\"{i} = {hparams.values()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4093f31e-49ad-4ceb-87f0-586681c0f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "iterator = MINDIterator\n",
    "model = NRMSModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a149ab9f-3527-430f-993f-ae93c557956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model without any training -- can training do better than no training at all?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18901dab-1155-4a46-a654-baf24cd6495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "663it [00:03, 214.87it/s]\n",
      "1143it [00:52, 21.79it/s]\n",
      "73152it [00:11, 6384.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.4621, 'mean_mrr': 0.1991, 'ndcg@5': 0.2, 'ndcg@10': 0.2625}\n"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(dev_news_file, dev_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da9eec7-a401-4302-84c9-3bed9b9c7654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 3690 , total_loss: 1.3599, data_loss: 1.1711: : 3693it [11:42,  5.26it/s]\n",
      "663it [00:01, 414.43it/s]\n",
      "1143it [00:48, 23.66it/s]\n",
      "73152it [00:10, 6982.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.3598220211169019\n",
      "eval info: group_auc:0.6345, mean_mrr:0.2886, ndcg@10:0.3827, ndcg@5:0.3148\n",
      "at epoch 1 , train time: 702.1 eval time: 151.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 3690 , total_loss: 1.2680, data_loss: 1.2221: : 3693it [11:25,  5.38it/s]\n",
      "663it [00:01, 421.26it/s]\n",
      "1143it [00:46, 24.55it/s] \n",
      "73152it [00:10, 7113.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.2680544555590723\n",
      "eval info: group_auc:0.645, mean_mrr:0.2976, ndcg@10:0.3929, ndcg@5:0.3268\n",
      "at epoch 2 , train time: 685.9 eval time: 149.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 3690 , total_loss: 1.2255, data_loss: 1.1623: : 3693it [11:26,  5.38it/s]\n",
      "663it [00:01, 441.54it/s]\n",
      "1143it [00:46, 24.50it/s]\n",
      "73152it [00:09, 7437.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.225447253647646\n",
      "eval info: group_auc:0.6431, mean_mrr:0.2938, ndcg@10:0.3912, ndcg@5:0.3251\n",
      "at epoch 3 , train time: 686.3 eval time: 149.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 3690 , total_loss: 1.1891, data_loss: 1.3220: : 3693it [11:24,  5.40it/s]\n",
      "663it [00:01, 420.21it/s]\n",
      "1143it [00:48, 23.68it/s]\n",
      "73152it [00:09, 7992.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.1890128452120001\n",
      "eval info: group_auc:0.6397, mean_mrr:0.2959, ndcg@10:0.3898, ndcg@5:0.3246\n",
      "at epoch 4 , train time: 684.4 eval time: 151.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 3690 , total_loss: 1.1511, data_loss: 0.9518: : 3693it [11:23,  5.40it/s]\n",
      "663it [00:01, 435.90it/s]\n",
      "1143it [00:47, 23.87it/s]\n",
      "73152it [00:10, 7220.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 5\n",
      "train info: logloss loss:1.151109272327134\n",
      "eval info: group_auc:0.6348, mean_mrr:0.2915, ndcg@10:0.3859, ndcg@5:0.3202\n",
      "at epoch 5 , train time: 683.3 eval time: 150.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommenders.models.newsrec.models.nrms.NRMSModel at 0x7f6ac91f2f20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_news_file, train_behaviors_file, dev_news_file, dev_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d410d89d-8fc6-411c-a9e8-eaa7fc454d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "663it [00:01, 371.54it/s]\n",
      "1143it [00:46, 24.69it/s]\n",
      "73152it [00:10, 7089.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.6348, 'mean_mrr': 0.2915, 'ndcg@5': 0.3202, 'ndcg@10': 0.3859}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(dev_news_file, dev_behaviors_file)\n",
    "print(res_syn)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0e61ee-7634-4706-844b-ec2f2b0ec61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.6348,
       "encoder": "json",
       "name": "group_auc"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "group_auc"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.2915,
       "encoder": "json",
       "name": "mean_mrr"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "mean_mrr"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.3202,
       "encoder": "json",
       "name": "ndcg@5"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "ndcg@5"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.3859,
       "encoder": "json",
       "name": "ndcg@10"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "ndcg@10"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record results for tests - ignore this cell\n",
    "store_metadata(\"group_auc\", res_syn['group_auc'])\n",
    "store_metadata(\"mean_mrr\", res_syn['mean_mrr'])\n",
    "store_metadata(\"ndcg@5\", res_syn['ndcg@5'])\n",
    "store_metadata(\"ndcg@10\", res_syn['ndcg@10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a310e45-b582-466e-92b4-dc4aa9c1bcd3",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1a4ca68-4b8e-4709-8ffe-b9cae7c71c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1f46d-1f22-436f-86eb-621a0fd23a39",
   "metadata": {},
   "source": [
    "## Output Prediction File\n",
    "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
    "\n",
    "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cccb15e2-c927-4c22-811e-3f33995326b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "663it [00:01, 353.47it/s]\n",
      "1143it [00:46, 24.70it/s]\n",
      "73152it [00:10, 7014.21it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(dev_news_file, dev_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aecd1134-5b08-4118-9220-44eab1c68c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:01, 54372.87it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f988a8c-5e8f-4962-bb5e-43a5b6ca3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f9ee5-146e-4c8b-97ad-59c885b46c4a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\n",
    "\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "793306ff-fe00-4fff-b7d3-776475b2fa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1:16:34\n"
     ]
    }
   ],
   "source": [
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Convert the elapsed time into hours, minutes, and seconds\n",
    "hours, remainder = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)   \n",
    "\n",
    "# Print the result in H:M:S format\n",
    "print(f\"Elapsed time: {int(hours)}:{int(minutes)}:{int(seconds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c1739-a12d-438b-b26e-07330ac38514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sb3)",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
