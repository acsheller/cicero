{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b654990-0011-404a-b3ab-695a7c1a3954",
   "metadata": {},
   "source": [
    "# CICERO -- NRMS Large Dataset Adapted\n",
    "\n",
    "This notebook is a run of the [NRMS notebook]( adapted from the [Recommenders Team](https://github.com/recommenders-team/recommenders/tree/main).  It has been modified as time has caused some things to not work as presneted originally.\n",
    "\n",
    "NRMS stands for `Neural News Reccomendaiton with Multi-head Self-Attention`.  The reference to the paper is provided below. please look over and study the [recommenders team github repository](https://github.com/recommenders-team/recommenders/blob/main/README.md#Getting-Started) because this code is derived from it. \n",
    "\n",
    "Read up on the [MIND: MIcrosoft News Dataset](https://msnews.github.io/) and download the data into the datasets folder of this project and place them in `/apps/datasets`.  Review the [Readme](/app/datasets/README.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4a4bb-acec-4343-a73a-a0b4da74b1ec",
   "metadata": {},
   "source": [
    "## do imports and check things out.\n",
    "\n",
    "Are we using a GPU? If not then things will take a very long time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629edf53-75e2-4492-86b4-19f7c6e14d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/app/sb3/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
      "Tensorflow version: 2.15.1\n",
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU is available and TensorFlow is using it.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove warnings\n",
    "import os\n",
    "os.environ['TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "# Gets the SUBERX python modules included.\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "# This module was created to generate the necessary files if they dont' already exist\n",
    "from environment.data_utils import generate_uid2index, load_glove_embeddings, create_word_dict_and_embeddings, setup_nltk_resources\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.datasets.mind import download_and_extract_glove\n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# List available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Check if a GPU is detected\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. TensorFlow is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097e01d-d996-4abb-ae37-77b2a1e37c2c",
   "metadata": {},
   "source": [
    "## Prepare Parameters\n",
    "\n",
    "Adjust these as needed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3910c820-6995-4335-94d9-89163dc7254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# My modification for MINDSMALL as I mount in the data into the container.  Further the small dataset is not accessable anymore.\n",
    "\n",
    "# Options: MINDdemo, MINDsmall, MINDlarge\n",
    "\n",
    "MIND_type = 'MINDlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a3bf7-ed24-4a13-bbc0-6238aa1ced5c",
   "metadata": {},
   "source": [
    "## Specify the dataset to use\n",
    "\n",
    "There are three in the original notebook: `demo`, `small` and `large`.\n",
    "\n",
    "From the [original  notebook](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb),  the `demo` set is 5000 samples of the `small` dataset.\n",
    "\n",
    "I was able to download the original demo but that's not really needed. A sample of 5000 is good for showing the algorithm works but won't train it well enough.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dda3b7-fe49-4951-b083-51476b46d556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path is /app/datasets/MINDlarge\n"
     ]
    }
   ],
   "source": [
    "## This is how it was done originally\n",
    "#tmpdir = TemporaryDirectory()\n",
    "#data_path = tmpdir.name\n",
    "\n",
    "## I mount in the datasets folder to /app\n",
    "data_path_base=\"/app/datasets/\"\n",
    "data_path = data_path_base + MIND_type\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data Path is {data_path}\")\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "dev_news_file = os.path.join(data_path,'dev','news.tsv')\n",
    "dev_behaviors_file = os.path.join(data_path,'dev','behaviors.tsv')\n",
    "\n",
    "wordEmb_file = os.path.join(data_path, 'utils',\"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, 'utils',\"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, 'utils',\"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\",'nrms.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057c90c-fe0e-4804-85f0-e5b696b5b832",
   "metadata": {},
   "source": [
    "## Download Glove embeddings\n",
    "\n",
    "The original NRMS used glove embeddings, this is how they are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "752fc015-8d01-4f2d-957a-e8997ad8155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings extracted to: /app/datasets/glove_embeddings\n"
     ]
    }
   ],
   "source": [
    "## All models can use the same glove embeddings\n",
    "\n",
    "# Download and extract GloVe embeddings\n",
    "glove_dir = data_path_base +'glove_embeddings'\n",
    "download_and_extract_glove(glove_dir)\n",
    "print(f\"GloVe embeddings extracted to: {glove_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fa50e-08c4-4018-bb3f-10f30ead7a1d",
   "metadata": {},
   "source": [
    "## Get the NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f67009f-586a-49c4-bc8e-b88f6576114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded to /app/datasets/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /app/datasets/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /app/datasets/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_data_dir = data_path_base + 'nltk_data'\n",
    "setup_nltk_resources(download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b1b40-1b4b-4997-bc6a-aa15dbb8f7b5",
   "metadata": {},
   "source": [
    "## Identify the news and behavior files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf258e6-0ce3-4cfc-9df9-c2fd2296089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /app/datasets/MINDlarge/utils/uid2index.pkl with 711222 users.\n"
     ]
    }
   ],
   "source": [
    "# Generate uid2index.pkl\n",
    "uid2index = generate_uid2index(train_behaviors_file, output_file= userDict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8964f2d2-b308-4cfa-96c1-eed675ebd554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_embeddings.pkl already exists. Loading embeddings from file.\n"
     ]
    }
   ],
   "source": [
    "# Path to GloVe file (e.g., glove.6B.300d.txt)\n",
    "glove_file = os.path.join(glove_dir, \"glove/glove.6B.300d.txt\")\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_file, embedding_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5deb0bf-ef42-4b32-934b-ef63a7c66374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing text: 100% 101527/101527 [00:32<00:00, 3152.71it/s]\n",
      "Creating embedding matrix: 100% 101242/101242 [00:00<00:00, 159118.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved word_dict.pkl and embedding.npy to /app/datasets/MINDlarge/utils/.\n"
     ]
    }
   ],
   "source": [
    "# Generate word_dict.pkl and embedding.npy\n",
    "word_dict, embedding_matrix = create_word_dict_and_embeddings(\n",
    "    news_file= train_news_file,\n",
    "    glove_embeddings=glove_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dir= data_path + '/utils/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9ca8c-8424-4d0d-919d-460cd0183cd7",
   "metadata": {},
   "source": [
    "## Define the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e1f690-4b13-4b82-af03-e50d095cdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(None, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          model_type=\"nrms\",\n",
    "                          title_size=30,\n",
    "                          his_size=50,\n",
    "                          npratio=4,\n",
    "                          data_format='news',\n",
    "                          word_emb_dim=300,\n",
    "                          head_num=20,\n",
    "                          head_dim=20,\n",
    "                          attention_hidden_dim=200,\n",
    "                          loss='cross_entropy_loss',\n",
    "                          dropout=0.2,\n",
    "                          support_quick_scoring=True,\n",
    "                          show_step=10, \n",
    "                          metrics=['group_auc', 'mean_mrr', 'ndcg@5;10'])\n",
    "#print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2b302a2-efd9-4ff9-9109-ea7f202bd902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support_quick_scoring = True\n",
      "dropout = 0.2\n",
      "attention_hidden_dim = 200\n",
      "head_num = 20\n",
      "head_dim = 20\n",
      "filter_num = 200\n",
      "window_size = 3\n",
      "vert_emb_dim = 100\n",
      "subvert_emb_dim = 100\n",
      "gru_unit = 400\n",
      "type = ini\n",
      "user_emb_dim = 50\n",
      "learning_rate = 0.001\n",
      "optimizer = adam\n",
      "epochs = 5\n",
      "batch_size = 64\n",
      "show_step = 10\n",
      "wordEmb_file = /app/datasets/MINDlarge/utils/embedding.npy\n",
      "wordDict_file = /app/datasets/MINDlarge/utils/word_dict.pkl\n",
      "userDict_file = /app/datasets/MINDlarge/utils/uid2index.pkl\n",
      "model_type = nrms\n",
      "title_size = 30\n",
      "his_size = 50\n",
      "npratio = 4\n",
      "data_format = news\n",
      "word_emb_dim = 300\n",
      "loss = cross_entropy_loss\n",
      "metrics = ['group_auc', 'mean_mrr', 'ndcg@5;10']\n"
     ]
    }
   ],
   "source": [
    "for i in hparams.values().keys():\n",
    "    print(f\"{i} = {hparams.values()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b0b520a-cc6f-4159-ad45-95f4d66ba03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "iterator = MINDIterator\n",
    "model = NRMSModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f8b91-cbde-497c-bd88-6099ea103dea",
   "metadata": {},
   "source": [
    "### Run the model without any training -- can training do better than no training at all?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "372616ae-8fc7-4473-a54d-d56ebf0b67bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/sb3/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "1126it [00:04, 240.65it/s]\n",
      "5883it [04:27, 21.97it/s]\n",
      "376471it [01:16, 4945.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.4628, 'mean_mrr': 0.2005, 'ndcg@5': 0.2019, 'ndcg@10': 0.2641}\n"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(dev_news_file, dev_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687164d4-ccf8-4e47-8954-5d51bd7a03df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 52870 , total_loss: 1.2507, data_loss: 1.0388: : 52870it [2:50:47,  5.16it/s]\n",
      "1126it [00:02, 421.61it/s]\n",
      "5883it [04:06, 23.82it/s]\n",
      "376471it [01:12, 5184.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.2507364407173285\n",
      "eval info: group_auc:0.6716, mean_mrr:0.317, ndcg@10:0.4158, ndcg@5:0.3521\n",
      "at epoch 1 , train time: 10247.3 eval time: 805.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 52870 , total_loss: 1.2107, data_loss: 1.1084: : 52870it [2:47:04,  5.27it/s]\n",
      "1126it [00:02, 423.83it/s]\n",
      "5883it [04:05, 23.93it/s]\n",
      "376471it [01:42, 3656.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.210744759015963\n",
      "eval info: group_auc:0.6602, mean_mrr:0.3094, ndcg@10:0.4069, ndcg@5:0.342\n",
      "at epoch 2 , train time: 10024.7 eval time: 825.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 52870 , total_loss: 1.2359, data_loss: 1.2254: : 52870it [2:47:17,  5.27it/s]\n",
      "1126it [00:02, 452.32it/s]\n",
      "5883it [04:05, 23.92it/s]\n",
      "376471it [00:57, 6580.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.2359015685287207\n",
      "eval info: group_auc:0.6398, mean_mrr:0.2897, ndcg@10:0.3864, ndcg@5:0.3191\n",
      "at epoch 3 , train time: 10037.8 eval time: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 52870 , total_loss: 1.3391, data_loss: 1.6094: : 52870it [2:46:18,  5.30it/s] \n",
      "1126it [00:02, 449.76it/s]\n",
      "5883it [04:03, 24.15it/s]\n",
      "376471it [00:54, 6915.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.3390802287854746\n",
      "eval info: group_auc:0.5367, mean_mrr:0.2237, ndcg@10:0.2989, ndcg@5:0.2383\n",
      "at epoch 4 , train time: 9978.1 eval time: 774.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 52870 , total_loss: 1.6223, data_loss: 1.6094: : 52870it [2:45:54,  5.31it/s] \n",
      "1126it [00:02, 452.79it/s]\n",
      "5883it [04:03, 24.16it/s]\n",
      "376471it [00:55, 6826.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 5\n",
      "train info: logloss loss:1.6222897745112244\n",
      "eval info: group_auc:0.5269, mean_mrr:0.2306, ndcg@10:0.3024, ndcg@5:0.2401\n",
      "at epoch 5 , train time: 9954.1 eval time: 776.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommenders.models.newsrec.models.nrms.NRMSModel at 0x7fbc941e2560>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04aa2358-4cc6-4d0d-9083-327f5443a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1126it [00:02, 404.73it/s]\n",
      "5883it [04:02, 24.23it/s]\n",
      "376471it [00:55, 6766.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.5269, 'mean_mrr': 0.2306, 'ndcg@5': 0.2401, 'ndcg@10': 0.3024}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(dev_news_file, dev_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdfb893-0865-4407-a2d1-39ac743a75ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.5269,
       "encoder": "json",
       "name": "group_auc"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "group_auc"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.2306,
       "encoder": "json",
       "name": "mean_mrr"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "mean_mrr"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.2401,
       "encoder": "json",
       "name": "ndcg@5"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "ndcg@5"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": 0.3024,
       "encoder": "json",
       "name": "ndcg@10"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "ndcg@10"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record results for tests - ignore this cell\n",
    "store_metadata(\"group_auc\", res_syn['group_auc'])\n",
    "store_metadata(\"mean_mrr\", res_syn['mean_mrr'])\n",
    "store_metadata(\"ndcg@5\", res_syn['ndcg@5'])\n",
    "store_metadata(\"ndcg@10\", res_syn['ndcg@10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7a2e9-78f0-46f8-8371-675660871f3f",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f44696c6-c043-4a03-801e-af7f539627bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71ace3-d1c8-4c42-ba80-bff2715a050e",
   "metadata": {},
   "source": [
    "## Output Prediction File\n",
    "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
    "\n",
    "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfb3aacf-eff4-4491-a225-5f16230f3666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1126it [00:02, 399.75it/s]\n",
      "5883it [04:03, 24.20it/s]\n",
      "376471it [00:55, 6812.36it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f31ecb1-5491-44f2-9f92-dedd3314b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "376471it [00:06, 59852.85it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30636f-7154-431e-8d2c-e72c46a41249",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\n",
    "\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3c4745-fe10-4976-ba58-9f15d307f1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 15:36:51\n"
     ]
    }
   ],
   "source": [
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Convert the elapsed time into hours, minutes, and seconds\n",
    "hours, remainder = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "# Print the result in H:M:S format\n",
    "print(f\"Elapsed time: {int(hours)}:{int(minutes)}:{int(seconds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a895aba-e8bf-4fd4-b66a-916134d52a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sb3)",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
